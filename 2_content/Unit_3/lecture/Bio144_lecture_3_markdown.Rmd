---
title: "Kurs Bio144: Datenanalyse in der Biologie"
subtitle: "Lecture 3: Simple linear regression"
author: "Stefanie Muff (Lecture) & Owen L.Petchey (Practical)"
institute: "University of Zurich"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  beamer_presentation:
  includes:
  in_header: ../../beamer_stuff/preamble.tex
classoption: "aspectratio=169"
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(xtable.comment = FALSE)
```

## Overview

* Introduction of the linear regression model
* Parameter estimation
* Simple model checking
* Goodness of the model: Correlation and $R^2$
* Tests and confidence intervals
* Confidence and prediction ranges

## Course material covered today

The lecture material of today is based on the following literature:

\ 

* Chapter 2 of _Lineare Regression_, p.7-20 (Stahel script)

## The body fat example

\

Remember: Aim is to find prognostic factors for body fat, without actually measuring it.
Even simpler question: How good is BMI as a predictor for body fat?

\
```{r echo = FALSE, eval=TRUE}
path <- "../../../3_datasets/"
d.bodyfat <- read.table(paste(path,"bodyfat.clean.txt",sep=""),header=T)
d.bodyfat <- d.bodyfat[,c("bodyfat","age","gewicht","hoehe","bmi","neck","abdomen","hip")]
```

```{r eval=TRUE, fig.width=3.5, fig.height=3, out.width='5cm', fig.align='center', warning=FALSE, echo = FALSE}
library(ggplot2)
ggplot(d.bodyfat,aes(x=bmi,y=bodyfat)) + geom_point() + 
  theme_bw() + ylab("body fat (%)")
```

## Linear relationship

* The most simple relationship between an \emph{explanatory variable} ($X$) and a \emph{target/outcome variable} ($Y$) is a linear relationship. All points $(x_i,y_i)$, $i= 1,\ldots, n$, on a  straight line follow the equation
$$y_i = \alpha + \beta x_i\ .$$

* Here, $\alpha$ is the \alert{axis intercept} and $\beta$ the \alert{slope} of the line. $\beta$ is also denoted as the regression coefficient of $X$.

* If $\alpha=0$ the line goes through the origin $(x,y)=(0,0)$.

* \alert{Interpretation} of linear dependency: proportional increase in $y$ with increase (decrease) in $x$.

## 

But which is the "true" or "best" line?

```{r eval=TRUE, fig.width=3.5, fig.height=3, warning=FALSE, echo = FALSE, out.width='6.5cm', fig.align='center'}
ggplot(d.bodyfat,aes(x=bmi,y=bodyfat)) + geom_point() + theme_bw() + ylab("body fat (%)")  + 
  geom_abline(intercept = -25, slope = 1.7, color="red",   size=0.6) +
  geom_abline(intercept = -35, slope = 2.1, color="green",    size=0.6) +
geom_abline(intercept = -36, slope = 2.25, color="blue",    size=0.6) 
```

__Task:__ Estimate the regression parameters $\alpha$ and $\beta$ (by "eye") and write them down. 

## 

\

It is obvious that 

* the linear relationship does not describe the data perfectly
* another realization of the data (other 243 males) would lead to a slightly different picture.

\

$\Rightarrow$ We need a __model__ that describes the relationship between BMI and bodyfat.

## The simple linear regression model

\colorbox{lightgray}{\begin{minipage}{14cm}
In the linear regression model the dependent variable $Y$ is related to the independent variable $x$ as
$$Y = \alpha + \beta x + \epsilon \ , \qquad \epsilon \sim N(0,\sigma^2)$$
\end{minipage}}

In this formulation $Y$ is a random variable $Y \sim N(\alpha + \beta x, \sigma^2$) where
$$Y \quad= \quad \underbrace{\text{expected value}}_{E(Y) = \alpha + \beta x} \quad + \quad \underbrace{\text{random error}}_{\epsilon}  \ .$$
Note:

* The model for $Y$ given $x$ has \alert{three parameters}: $\alpha$, $\beta$ and $\sigma^2$ .
* $x$ is the \alert{independent} / \alert{explanatory} / \alert{regressor} variable.
* $Y$ is the \alert{dependent} / \alert{outcome} / \alert{response} variable.

## 

__Note__

* The linear model propagates the most simple relationship between two variables. When using it, please always think if such a relationship is meaningful/reasonable/plausible.
* Always look at the data \alert{before} you start with model fitting.


## Visualization of the regression assumptions

The assumptions about the linear regression model lie in the error term $$\epsilon \sim N(0,\sigma^2) \ . $$


<!--\includegraphics[width=11cm]{pictures/regrAssumptions.jpg}-->
Note: The true regression line goes through $E(Y)$.

## Insights from data simulation

\scriptsize (Simulation are \emph{always} a great way to understand statistics!!)

Generate an independent (explanatory) variable __x__ and __two__ samples of a dependent variable __y__ assuming that
$$y_i = 4 - 2x_i + \epsilon_i \ , \quad \epsilon_i\sim N(0,0.5^2) \ .$$

```{r eval=T, fig.width=3.5, fig.height=3.3, out.width='5cm', echo = FALSE, fig.align='center'}
set.seed(134539)
par(mar=c(4,4,1,1))
x <- runif(25,-2,2)
y1 <- 4 - 2*x + rnorm(25,0,sd=0.5)
y2 <- 4 - 2*x + rnorm(25,sd=0.5)
plot(x,y1,ylim=c(min(c(y1,y2)),max(c(y1,y2))),ylab="y")
points(x,y2,col=2)
abline(c(4,-2))
legend("topright",legend=c("sample 1","sample 2"),col=1:2, pch=1)
```

$\rightarrow$ Random variation is always present. This leads us to the next question.

## Parameter estimation

In a regression analysis, the task is to estimate the \alert{regression coefficients} $\alpha$, $\beta$ and the \alert{residual variance} $\sigma^2$ for a given set of $(x,y)$ data.

* __Problem:__ For more than two points $(x_i,y_i)$, $i=1,\ldots, n$, there is generally no perfectly fitting line.

* __Aim:__ We want to find the parameters $(a,b)$ of the best fitting line $Y = a + b x$.

* __Idea:__ Minimize the deviations between the data points $(x_i,y_i)$ and the regression line.

\
But how? 

## Should we minimize these distances...

```{r eval=T,fig.width=5.7, fig.height=5,warning=FALSE,out.width="10cm", echo = FALSE, fig.align='center'}
set.seed(9670)
n <- 10
x <- rnorm(n)
y <- 4 - 2*x + rnorm(n,0,sd=1)
x[11]  <- -0.5
y[11] <- 6.2
plot(x,y)
x1 <- -0.66
abline(c(4,-2),lwd=2)
segments(x[11],y[11],x1,4-2*(x1),col=2,lwd=2)
```

## Or these?

```{r eval=T,fig.width=5.7, fig.height=5,warning=FALSE,out.width="10cm", echo = FALSE, fig.align='center'}
set.seed(9670)
n <- 10
x <- rnorm(n)
y <- 4 - 2*x + rnorm(n,0,sd=1)
x[11]  <- -0.5
y[11] <- 6.2
plot(x,y)
abline(c(4,-2),lwd=2)
segments(x[11],y[11],x[11],4-2*x[11],col=2,lwd=2)
```

## Or maybe even these?

```{r eval=T,fig.width=5.7, fig.height=5,warning=FALSE,out.width="10cm", echo = FALSE, fig.align='center'}
set.seed(9670)
n <- 10
x <- rnorm(n)
y <- 4 - 2*x + rnorm(n,0,sd=1)
x[11]  <- -0.5
y[11] <- 6.2
plot(x,y)
abline(c(4,-2),lwd=2)
segments(x[11],y[11],(y[11]-4)/(-2),y[11],col=2,lwd=2)
```

## Least squares

For multiple reasons (theoretical aspects and mathematical convenience), the parameters are estimated using the \alert{least squares} approach. In this, yet something else is minimized:

\colorbox{lightgray}{\begin{minipage}{14cm}
The parameters $\alpha$ and $\beta$ are estimated such that the sum of \alert{squared vertical distances} (sum of squared residuals)

$$SSE = \sum_{i=1}^n e_i^2 \ , \qquad \text{where} \quad e_i = y_i - \underbrace{(a + b x_i)}_{=\hat{y}_i} $$

is being minimized.
\end{minipage}}

__Note:__ $\hat y_i = a + b x_i$ are the \alert{predicted values}.

## So we minimize the sum of these areas!

```{r eval=T,fig.width=5.7, fig.height=5,warning=FALSE,out.width="10cm", fig.align='center', echo=FALSE}
set.seed(9670)
n <- 10
x <- rnorm(n)
y <- 4 - 2*x + rnorm(n,0,sd=1)
x[11]  <- -0.5
y[11] <- 6.2
dd <- 0.38
from_x <- c(x[11],x[11],x[11]+dd,x[11] + dd) 
from_y <- c(y[11],(4-2*x[11]),(4-2*x[11]),y[11])

to_x <- c(x[11],x[11] + dd,x[11]+ dd,x[11])
to_y <- c(4-2*x[11],4-2*x[11],y[11], y[11])

plot(x,y)
abline(c(4,-2),lwd=2)
polygon(from_x,from_y,to_x,to_y,col=2,lwd=2)
```

## Least squares estimates

For a given sample $(x_i,y_i), i=1,..,n$, with mean values $\overline{x}$ and $\overline{y}$, the least squares estimates $\hat\alpha$ and $\hat\beta$ are computed as 

\colorbox{lightgray}{\begin{minipage}{14cm}
\begin{eqnarray*}
\hat\beta &=& \frac{\sum_{i=1}^n  (y_i - \overline{y}) (x_i - \overline{x})}{ \sum_{i=1}^n (x_i - \overline{x})^2 } = \frac{cov(x,y)}{var(x)} \ , \\[4mm]
\hat\alpha &=& \overline{y} - \hat\beta \overline{x}  \ .
\end{eqnarray*}
\end{minipage}}

Moreover,

\colorbox{lightgray}{\begin{minipage}{14cm}
\begin{eqnarray*}
\hat\sigma^2 &=& \frac{1}{n-2}\sum_{i=1}^n e_i^2 \quad \text{with residuals  } e_i = y_i - (\hat\alpha + \hat\beta x_i)
\end{eqnarray*}
\end{minipage}}

is an unbiased estimate of the residual variance $\sigma^2$.

\small (The derivation of the parameters can be looked up in the Stahel script 2.A b. Idea: Minimization through derivating equations and setting them =0.)

## Do-it-yourself "by hand"

Go to the Shiny gallery and try to ``estimate'' the correct parameters.

You can do this here:

\url{https://gallery.shinyapps.io/simple_regression/}

## Estimation using R

Let's estimate the regression parameters from the bodyfat example

\tiny
```{r echo=T,eval=T}
r.bodyfat <- lm(bodyfat ~ bmi,d.bodyfat)
summary(r.bodyfat)
```

<!--$\Rightarrow$ $\hat\alpha = $ `r round(r.bodyfat$coef[1],2)` ,  $\hat\beta=$ `r round(r.bodyfat$coef[2],2)`, $\hat\sigma_e =$ 
`r round(summary(r.bodyfat)$sigma,2)`-->

## 

The resulting line can be added to the scatterplot:

```{r echo = FALSE, message=FALSE, warning=FALSE}
library(reporttools)
library(biostatUZH)
```

```{r eval=T,fig.width=3.5, fig.height=3,warning=FALSE,echo=F,out.width="6.5cm", fig.align='center', message=FALSE, warning=FALSE}
ggplot(d.bodyfat,aes(bmi,bodyfat)) + geom_point() + geom_smooth(method='lm',se=F) + theme_bw()
```

\underline{Interpretation:} for an increase in the BMI by one index point, we roughly expect a 1.82\% percentage increase in bodyfat.

## Uncertainty in the estimates $\hat\alpha$ and $\hat\beta$

Important: $\hat\alpha$ and $\hat\beta$ are themselves \alert{random variables} and as such contain \alert{uncertainty}!

Let us look again at the regression output, this time only for the coefficients. The second column shows the standard error of the estimate:

\ 
\tiny
```{r echo=T,eval=T}
summary(r.bodyfat)$coef

```

\
 
\normalsize
$\rightarrow$ The logical next question is: what is the distribution of the estimates?


## Distribution of the estimators for $\hat\alpha$ and $\hat\beta$

To obtain an idea, we generate data points according to model

$$y_i = 4 - 2x_i + \epsilon_i \ , \quad \epsilon_i\sim N(0,0.5^2). $$
In each round, we estimate the parameters and store them:

```{r echo = FALSE, eval = TRUE}
set.seed(1)
```

\tiny
```{r echo = T, eval = T}
niter <- 1000
pars <- matrix(NA,nrow=niter,ncol=2)
for (ii in 1:niter){
  x <- rnorm(100)
  y <- 4 - 2*x + rnorm(100,0,sd=0.5)
  pars[ii,] <- lm(y~x)$coef
}
```

\normalsize
Doing it 1000 times, we obtain the following distributions for $\hat\alpha$ and $\hat\beta$:

```{r eval=T,fig.width=6, fig.height=3,warning=FALSE,out.width="5cm", echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
library(cowplot)
pars <- data.frame(pars)
names(pars) <- c("alpha","beta")
 
p1 <-  ggplot(pars,aes(x=alpha)) + geom_histogram() +  theme_bw()
p2 <-  ggplot(pars,aes(x=beta)) + geom_histogram() +  theme_bw()
p <- plot_grid(p1,p2,  ncol = 2, rel_heights = c(1, .2))
p
```

## 

This looks suspiciously normal!

In fact, from theory it is known that  

\begin{eqnarray*}
\hat\beta \sim N(\beta,{\sigma^{(\beta)2}}) & \quad \text{and} \quad & \hat\alpha \sim N(\alpha,{\sigma^{(\alpha)2}})
\end{eqnarray*}

For formulas of the standard deviations ${\sigma^{(\beta)2}}$ and ${\sigma^{(\alpha)2}}$, please consult Stahel 2.2.h.

\colorbox{lightgray}{\begin{minipage}{14cm}
{\bf To remember:}
\begin{itemize}
\item $\hat\alpha$ and $\hat\beta$ are \alert{unbiased estimators} of $\alpha$ and $\beta$.
\item the parameters estimates $\hat\alpha$ and $\hat\beta$ are \alert{normally distributed.}
\item the formulas for the variances depend on the residual variance $\sigma^2$, the sample size $n$ and the variability of $X$ (SSQ$^{(X)(\star)})$.
\end{itemize}
\end{minipage}}

$$^{(\star)}$$  $$\text{SSQ}^{(X)} = \sum_{i=1}^n (x_i-\overline{x})^2 $$

## Are the modelling assumptions met?

In practice, it is advisable to check if all our \alert{modelling assumptions are met}.

$\rightarrow$ Otherwise we might draw invalid conclusions from the results.

Remember: Our assumption is that $\epsilon_i \sim N(0,\sigma^2)$. This implies

\colorbox{lightgray}{\begin{minipage}{10cm}
\begin{enumerate}[a)]
\item The expected value of $\epsilon_i$ is 0: $E(\epsilon_i)=0$.
\item All $\epsilon_i$ have the same variance: $Var(\epsilon_i)=\sigma^2.$
\item All $\epsilon_i$ are normally distributed.
\end{enumerate}
\end{minipage}}

In addition, it is assumed that

\colorbox{lightgray}{\begin{minipage}{14cm}
\begin{enumerate}[d)]
\item  $\epsilon_1, \epsilon_2, \ldots, \epsilon_n$ are independent.
\end{enumerate}
\end{minipage}}

__Note:__ We do not actually observe $\epsilon_i$, but only the residuals $e_i$.
Let us introduce two simple graphical model checking tools for our residuals $e_i$.

## Model checking tool II: Histogram of residuals

Look at the histogram of the residuals:

```{r fig.width=4,fig.height=4,echo=FALSE,out.width='5.5cm', fig.align='center'}
hist(r.bodyfat$residuals,nclass=20,xlab="Residuals",main='')
```

The normal distribution assumption (c) seems ok as well.

## How good is the regression model?

This is, per se, a difficult question....

One often considered index is the __coefficient of determination (Bestimmtheitsmass)__ $R^2$.
Let us again look at the regression output form the bodyfat example:

```{r echo=TRUE, eval = TRUE}
summary(r.bodyfat)$r.squared
```

Compare this to the squared correlation between the two variables:

```{r echo = TRUE, eval = TRUE}
cor(d.bodyfat$bodyfat,d.bodyfat$bmi)^2
```


\colorbox{lightgray}{\begin{minipage}{10cm}
$\rightarrow$ In simple linear regression, $R^2$ is the squared correlation between the independent and the dependent variable.
\end{minipage}}

##

\colorbox{lightgray}{\begin{minipage}{14cm}
\begin{itemize} 
\item $R^2$ indicates the proportion of variability of the response variable ${y}$ that is \textbf{explained by the ensemble of all covariates}. 
\item Its value lies between 0 and 1.
\end{itemize}
\end{minipage}}  

\

The __larger__ $R^2$  
\
$\Rightarrow$  the \textbf{more} variability of ${y}$ is captured ("explained") by the covariate  
$\Rightarrow$ the \textbf{"better"} is the model.  

\
\scriptsize(However, it's a bit more complicated, see later in the course...)

## Testing and Confidence Intervals

After the regression parameters and their uncertainties have been estimated, there are typically two fundamental questions:  

1. __"Are the parameters compatible with some specific value?"__  
\
Typically, the question is whether the slope $\beta$ might be 0 or not, that is: "Is there an effect of the covariate $x$ or not?"  
\
$\Rightarrow$ This leads to a \alert{\bf statistical test}.  
\
2. __"Which values of the parameters are compatible with the data?"__  
\
$\Rightarrow$ This leads us to determine \alert{\bf confidence intervals}.

## 

Let's first go back to the output from the bodyfat example: 

```{r echo = TRUE, eval = TRUE}
summary(r.bodyfat)$coef
```
\  
Besides the estimate and the standard error (which we discussed before), there is a \alert{\texttt{t value}} and a probability \alert{\texttt{Pr(>|t|)}} that we need to understand.

How do these things help us to answer the two questions above?

## Testing the effect of a covariate  

Remember: in a statistical test you first need to specify the \emph{null hypothesis}. Here, typically, the null hypothesis is

\colorbox{lightgray}{\begin{minipage}{14cm}
$$H_0: \quad \beta = \beta_0 =  0  \ .$$
\begin{center}
In words: $H_0$ =   "no effect" \\
\end{center}
\end{minipage}}

\small (Included in $H_0$ is the assumption that the data follow the simple linear regression model!)  
\

Here, the \emph{alternative hypothesis} is given by
\colorbox{lightgray}{\begin{minipage}{14cm}
$$H_A: \quad \beta \neq  0  $$
\end{minipage}}

## 

Remember: To carry out a statistical test, we need a \emph{test statistic}.

\begin{center}
\colorbox{lightgray}{\begin{minipage}{5cm}
\begin{center}
What is a test statistic? %\\[6mm]%(Clicker exercise!)
\end{center}
\end{minipage}}
\end{center}

\vspace{2mm}
$\rightarrow$ It is some type of \textbf{summary statistic} that follows a known distribution under $H_0$. For our purpose, we use the so-called \textbf{$T$-statistic} 

\begin{center}
\colorbox{lightgray}{\begin{minipage}{5cm}
\begin{equation}\label{eq:beta}
T=\frac{\hat\beta - \beta_0}{se^{(\beta)}}\ . %\quad \text{with} \quad se^{(\beta)}=\sqrt{\hat\sigma_e^2/SSQ^{(X)}}  \ .
\end{equation}
\end{minipage}}
\end{center}

Again: typically, $\beta_0=0$, so the formula simplifies to $T=\frac{\hat\beta}{se^{(\beta)}}$.

Under $H_0$, $T$ has a $t$-distribution with $n-2$ degrees of freedom ($n=$ number of data points).

\small (You should try to recall the t-distribution. Check Mat183, keyword: t-test.)

## 

So let's again go back to the bodyfat regression output:

```{r echo = TRUE, eval = TRUE}
summary(r.bodyfat)$coef
```
\
Task: 
\
$\rightarrow$ Please use equation \eqref{eq:beta} to find out how the first three columns (Estimate, Std. Error and t value) are related! Check by a calculation...

Note: The last column contains the \textbf{$p$-value} of the test $\beta=0$.

## Recap: Formal definition of the $p$-value

\colorbox{lightgray}{\begin{minipage}{14cm}
The \textbf{formal definition of $p$-value} is the probability to observe a data summary (e.g., an average) that is at least as extreme as the one observed, given that the Null Hypothesis is correct.
\end{minipage}}  
\
Example (normal distribution): Assume the observed test-statistic leads to a $z$-value = -1.96  
\
$\Rightarrow$ $Pr(|z|\geq 1.96)=0.05$ and $Pr(z\leq-1.96)=0.025$.

```{r eval=TRUE, fig.align='center',out.width="6cm",echo = FALSE}
par(mfrow=c(1,2))

zz1 <- qnorm(0.025)
zz2 <- qnorm(0.975)
zz3 <- qnorm(0.025)

cord.x1 <- c(-4,seq(-4,zz1,0.01),zz1)
cord.y1 <- c(0,dnorm(seq(-4,zz1,0.01)),0)

cord.x2 <- c(zz2,seq(zz2,4,0.01),4)
cord.y2 <- c(0,dnorm(seq(zz2,4,0.01)),0)

curve(dnorm(x,0,1),-4,4,ylab="density",main="Two-sided p-value (0.05)",xlab="")
polygon(cord.x1,cord.y1,col='gray')
polygon(cord.x2,cord.y2,col='gray')
text(-3,0.05,labels="2.5%")
text(3,0.05,labels="2.5%")

cord.x3 <- c(-4,seq(-4,zz3,0.01),zz3)
cord.y3 <- c(0,dnorm(seq(-4,zz3,0.01)),0)

curve(dnorm(x,0,1),-4,4,ylab="density",main="One-sided p-value (0.025)",xlab="")
polygon(cord.x3,cord.y3,col='gray')
text(-3,0.05,labels="2.5%")
```

## 

The regression output on slide 33 indicates that the $p$-value for BMI is very small ($p<0.0001$).

Conclusion: there is __very strong evidence__ that the BMI is associated with bodyfat, because $p$ is extremely small (thus it is very unlikely that such a slope $\hat\beta$ would be seen if there was no effect of BMI on body fat).

This basically answers question 1 from slide 29.

## A cautionary note on the use of $p$-values

Maybe you have seen that in statistical testing, often the criterion $p\leq 0.05$ is used to test whether $H_0$ should be rejected. This is often done in a black-or-white manner.

\colorbox{lightgray}{\begin{minipage}{14cm}
However, we will put a lot of attention to a more reasonable and cautionary interpretation of $p$-values in this course! 
\end{minipage}}

## Confidence intervals of regression parameters

Question 2 from slide 29:

\colorbox{lightgray}{\begin{minipage}{14cm}
However, we will put a lot of attention to a more reasonable and cautionary interpretation of $p$-values in this course! 
\end{minipage}}

To answer this question, we can determine the confidence intervals of the regression parameters.

__Facts we know about $\hat\beta$__

* $\hat\beta$ is estimated with a standard error of $\sigma^{(\beta)}$
* The distribution of $\hat\beta$ is normal, namely $\hat\beta\sim N(\beta,\sigma^{(\beta)2})$.
* However, since we need to estimate $\sigma^{(\beta)}$ from the data (the standard error), we have a $t$-distribution.

## 

Doing some calculations (similar to those in chapter 8.2.2 of Mat183 script) leads us to the 95\% confidence interval  

\begin{center}
\colorbox{lightgray}{\begin{minipage}{6cm}
$$[\hat\beta - c \cdot \hat\sigma^{(\beta)} ; \hat\beta + c \cdot \hat\sigma^{(\beta)}] \ ,$$
\end{minipage}}
\end{center}

where $c$ is the 97.5\% quantile of the $t$-distribution with $n-2$ degrees of freedom.

Doing this for the bodfat example "by hand" is not hard. We have 241 degrees of freedom:

```{r echo = TRUE, eval = TRUE}
coefs <- summary(r.bodyfat)$coef
beta <- coefs[2,1]
sdbeta <- coefs[2,2] 
beta + c(-1,1) * qt(0.975,241) * sdbeta 
```

## 

Even easier: directly ask R to give you the CIs.

```{r echo = TRUE, eval = TRUE}
confint(r.bodyfat,level=c(0.95))
```

In summary,
```{r results='asis', echo = FALSE}
tableRegression(r.bodyfat)
```

\underline{Interpretation:} for an increase in the bmi by one index point, roughly 1.82\% percentage points more bodyfat are expected, and all true values for $\beta$ between 1.61 and 2.03 are compatible with the observed data.

## Confidence and Prediction Ranges

* Remember: When another sample from the same population was taken, the regression line would look slightly different.  

* There are two questions to be asked:  


1. Which other regression lines are compatible with the observed data?
\
$\Rightarrow$ This leads to the \textbf{confidence range}.
 
2. Where do future observations with a given $x$ coordinate lie?
\
$\Rightarrow$ This leads to the \textbf{prediction range}.

## Bodyfat example

\begin{center}
```{r eval=TRUE, fig.width=5, fig.height=5,out.width="7cm", echo=FALSE}
t.range <- range(d.bodyfat$bmi)
t.xwerte <- seq(t.range[1]-1,t.range[2]+1,by=1)
t.vert <- predict(r.bodyfat,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="confidence")$fit
t.vorh <- predict(r.bodyfat,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="prediction")$fit
plot(d.bodyfat$bmi,d.bodyfat$bodyfat,main="",xlab="BMI",ylab="bodyfat",xlim=range(t.xwerte),ylim=c(-5,50),cex=0.8)
abline(r.bodyfat,lwd=2)
lines(x=t.xwerte,y=t.vert[,2],lty=8,lwd=2,col=2)
lines(x=t.xwerte,y=t.vert[,3],lty=8,lwd=2,col=2)
lines(x=t.xwerte,y=t.vorh[,2],lty=8,lwd=2,col=4)
lines(x=t.xwerte,y=t.vorh[,3],lty=8,lwd=2,col=4)
legend("bottomright", c("confidence range (95%)", "prediction range (95%)"),
lty=8, cex=1,col=c(2,4),lwd=2)
```
\end{center}

Note: The prediction range is much broader than the confidence range.

## Calculation of the confidence range

Given a fixed value of $x$, say $x_0$. The question is: 

\colorbox{lightgray}{\begin{minipage}{14cm}
Where does $\hat y_0 = \hat\alpha + \hat\beta x_0$ lie with a certain confidence (i.e., 95\%)? 
\end{minipage}}

This question is not trivial, because both $\hat\alpha$ and $\hat\beta$ are estimates from the data and contain uncertainty. 

The details of the calculation are given in Stahel 2.4b. 

\colorbox{lightgray}{\begin{minipage}{14cm}
Plotting the confidence interval around all $\hat y_0$ values one obtains the \textbf{confidence range} or \textbf{confidence band for the expected values} of $y$.
\end{minipage}}

Note: For the confidence range, only the uncertainty in the estimates $\hat\alpha$ and $\hat\beta$ matters.

## 


```{r eval=TRUE, fig.width=5,fig.height=5, out.width='7cm', echo = FALSE, fig.align='center'}
t.range <- range(d.bodyfat$bmi)
t.xwerte <- seq(t.range[1]-1,t.range[2]+1,by=1)
t.vert <- predict(r.bodyfat,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="confidence")$fit
t.vorh <- predict(r.bodyfat,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="prediction")$fit
plot(d.bodyfat$bmi,d.bodyfat$bodyfat,main="Confidence range",xlab="BMI",ylab="bodyfat",xlim=range(t.xwerte),ylim=c(-5,50),cex=0.8)
abline(r.bodyfat,lwd=2)
lines(x=t.xwerte,y=t.vert[,2],lty=8,lwd=2,col=2)
lines(x=t.xwerte,y=t.vert[,3],lty=8,lwd=2,col=2)
legend("bottomright", c("confidence range (95%)"),
lty=8, cex=1,col=c(2),lwd=2)
```


## Calculations of the prediction range

Given a fixed value of $x$, say $x_0$. The question is:  

\colorbox{lightgray}{\begin{minipage}{10cm}
Where does a {\bf future observation} lie with a certain confidence (i.e., 95\%)? 
\end{minipage}}

To answer this question, we have to \alert{consider not only the uncertainty in the predicted value} $\hat y_0 =  \hat\alpha + \hat\beta x_0$, but also the \alert{error in the equation $\epsilon_i \sim N(0,\sigma^2)$}.  

This is the reason why the {\bf prediction range is always wider than the confidence range}.

## 

```{r eval=T,fig.width=5,fig.height=5,out.width="7cm",echo=FALSE, fig.align='center'}
t.range <- range(d.bodyfat$bmi)
t.xwerte <- seq(t.range[1]-1,t.range[2]+1,by=1)
t.vert <- predict(r.bodyfat,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="confidence")$fit
t.vorh <- predict(r.bodyfat,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="prediction")$fit
plot(d.bodyfat$bmi,d.bodyfat$bodyfat,main="Prediction range",xlab="BMI",ylab="bodyfat",xlim=range(t.xwerte),ylim=c(-5,50),cex=0.8)
abline(r.bodyfat,lwd=2)
lines(x=t.xwerte,y=t.vorh[,2],lty=8,lwd=2,col=4)
lines(x=t.xwerte,y=t.vorh[,3],lty=8,lwd=2,col=4)
legend("bottomright", c( "prediction range (95%)"),
lty=8, cex=1,col=c(4),lwd=2)
```

## Tasks until the next practical (Thu/Fri)

The idea of the course is that as a preparation for the practical part you will do the following:

* Understand what today's lecture was about. You will certainly need to click through it again.
* Go to openedX and do all the "Before class (BC)" tasks.  


\bf $\rightarrow$ The same procedure applies to all course weeks.


