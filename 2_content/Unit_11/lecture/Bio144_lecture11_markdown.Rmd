---
title: "Kurs Bio144: Datenanalyse in der Biologie"
subtitle: "Lecture 11: Modeling binary data "
author: "Stefanie Muff (Lecture) & Owen L.Petchey (Practical)"
institute: "University of Zurich"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  beamer_presentation:
    includes:
      in_header: ../../beamer_stuff/preamble.tex
classoption: "aspectratio=169"
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(xtable.comment = FALSE)
```

## Overview

\begin{itemize}
\item Binary response variables
\item Contingency tables, $\chi^2$ test
\item Odds and (log) odds ratios
\item Logistic regression
\item Residual analysis / model checking / deviances
\item Interpretation of the results
\end{itemize}

```{r echo = FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(ggfortify)
```

## Course material covered today

The lecture material of today is based on the following literature:  
\  

\begin{itemize}
\item Repetition: Chapter 9.4 about $\chi^2$-Tests in the Luchsinger script \\[4mm]
\item Chapters 9.1 - 9.3 from \emph{The new statistics with R} (Hector book).\\[12mm]
\end{itemize}

Note that I have also uploaded the continuation of the Stahel script, chapters 7-9 that cover GLMs. This is \textbf{not} mandatory literature.

## Recap of last week: GLMs and Poisson regression

\begin{itemize}
\item We introduced \alert{generalized linear models} (GLMS) and key terms:\\
\hspace{1cm}{\bf Family} \hspace{1cm}{\bf Linear predictor}  \hspace{1cm}{\bf Link function}\\[4mm]

\item GLMs are useful when the response variable ${y}$ is not continuous \\
($\rightarrow$ residuals are not Gaussian).\\[4mm]

\item Count data usually lead to \alert{Poisson regression}.\\[4mm]
\end{itemize}

## Introduction

\begin{itemize}
\item Today, we will look at the case where the \alert{response variable is binary} (0 or 1) or \alert{binomial} (\emph{e.g.} 5 out of 7 trials).\\[2mm]
\item In binary/binomial regression, the question will be: "Which variables influence the \alert{probability} $p$ of the outcome?"\\[4mm]
\end{itemize}

\textbf{Examples:} 
\begin{itemize}
\item Outcome: Heart attack (yes=1, no=0). \\
Question: which variables lead to higher or lower risk of heart attack?\\[2mm]
\item Outcome: Survival (yes=1, no=0).\\
Question: which variables influence the survival probability of premature babies (Fr체hgeburten)?\\[2mm]
\end{itemize}

## Some repetition: The $\chi^2$ test

You have dealt with binary (categorical) data in Mat183! Remember the $\chi^2$ test for contingency tables (simplest: 2 x 2 tables).  
\  

Example: Heart attack and hormonal contraception (Verh체tungspille) (from Stahel):  
\  

<!--
\begin{center}
\includegraphics[width=6cm]{graphics/table1.png}
\end{center}
-->

\small 
"Hormonal contraception" is the predictor ($x$) and "heart attack" the outcome ($y$).  
\  

\normalsize
\textbf{Question:} Does hormonal contraception ($x$) have an influence on heart attacks ($y$)?

This question is \alert{equivalent to asking whether the proportion} of patients with heart attack \alert{is the same} in both groups.

## 

The respective test-statistic can be calculated as

\colorbox{lightgray}{\begin{minipage}{14cm}
\begin{equation*}
T = \sum_{\text{all entries}} \frac{(\text{observed} - \text{expected})^2}{\text{expected}} \ .
\end{equation*}
\end{minipage}}

By hand, $T$ is obtained as 


\begin{equation*}
\frac{(23 - 14.8)^2}{14.8} + \frac{(34 - 42.2)^2}{42.2} + \frac{(35 -  43.2)^2}{43.2} + \frac{(132 - 123.8)^2}{123.8} =  8.329
\end{equation*}  

\ 
and is expected to be $\chi^2_{1}$ distributed \tiny(one degree of freedom: $(2-1)\cdot(2-1)$)}.  
\ 

\normalsize
The $p$-value of this test is given as $\Pr(X\geq 8.329)=`r format(1-pchisq(8.329,1),4,4,4)`$.  
\  

\tiny
```{r echo = TRUE}
pchisq(8.329,1,lower.tail=F)
```

\normalsize

\bf $\rightarrow $ There is \alert{strong evidence} for an association of hormonal contraception with heart attacks!

## Quantification of a dependency

If two variables are not independent, it is often desired to \alert{quantify} the dependency.  
\  

Let one variable be the grouping variable (e.g., hormonal contraception vs no hormonal contraception). Then $\pi_1$ and $\pi_2$ are the relative frequencies (proportions) observed in the two groups. For example:  

\begin{eqnarray*}
\pi_1 = 23/57 &=& 0.404 \\
\pi_2 = 35/167 &=& 0.210\\
\end{eqnarray*}

are the proportions of females with a heart attack in the two groups.

## 

There are at least three numbers that can be calculated to quantify how the two groups differ:  
\  

\begin{itemize}
\item Risk difference: $\pi_1 - \pi_2 = 0.404 - 0.210$ = 0.194\\[4mm]

\item Relative risk: $\pi_1 / \pi_2 = 0.404 / 0.210 = 1.92 $\\[4mm]
\item \alert{Odds ratio} ("Chancenverh채ltnis"):
\begin{equation*}
OR = \frac{\pi_1 / (1-\pi_1)}{\pi_2 / (1-\pi_2)} = \frac{ 0.404 / (1- 0.404)}{0.210 / (1-0.210)} = 2.55 \ ,
\end{equation*}

where $\pi/(1-\pi)$ is the odds (die "Chance").  
\ 

Interpretation:
\begin{enumerate}
\item   $OR=1$  $\rightarrow$ the two groups are independent.
\item   $OR > 1 (<1) $ $\rightarrow$ positive (negative) dependency.
\end{enumerate}
\end{itemize}

## The odds and the odds ratio

\begin{itemize}
\item The {\bf odds} ("Wettverh채ltnis"): For a probability $\pi$ the odds is \begin{equation}
\frac{\pi}{(1-\pi)} = \frac{\text{Wahrscheinlichkeit}}{\text{Gegenwahrscheinlichkeit}} \ .
\end{equation}
For example, if the probability to win a game is 0.75, then the odds is given as 0.75/0.25 or 3:1.  

\item The {\bf odds ratio} is given on the previous slide. It is a ratio of two ratios, or, the {\bf ratio of two odds}.\\[4mm]

\item Often the {\bf log odds ratio} is used:
\begin{equation*}
\log(OR) = \log\left(\frac{\pi_1 / (1-\pi_1)}{\pi_2 / (1-\pi_2)} \right) \ .
\end{equation*}

\begin{enumerate}
\item   $\log(OR)=0$ $\rightarrow$ the two groups are independent.
\item   $\log(OR)>0 (<0)$ $\rightarrow$ positive (negative) dependency.
\end{enumerate}

\end{itemize}

## 

Please go to the following webpage for a short klicker exercise:  
\ 

\url{http://www.klicker.uzh.ch/bkx}

## Binomial and binary regression

Usually the situation is more complicated than 
\begin{center}\textbf{binary covariate} (${x}$) $\rightarrow$ \textbf{binary outcome} (${y}$)\end{center}
  
\  

Often, we are interested in a relationship 
\begin{center}  \textbf{Continuous/categ./binary} variables ${x}^{(1)}$, ${x}^{(2)}$,.. $\rightarrow$ \textbf{binary outcome} (${y}$) \\[10mm]
\end{center} 

$\rightarrow$ A regression model is needed again!

## Illustrative/working example

Let us look at an example from chapter 9.2 in Hector (2015): 

Eight groups of beetles were exposed to carbon disulphide (an insecticide) for 5h. For each beetle it was then reported if it was killed or not (1 or 0), but the data were reported in \textbf{aggregated} form:

\small
```{r echo = FALSE, message=FALSE, warning=FALSE}
library(AICcmodavg)
data(beetle)
beetle
```
\normalsize

## 

As always, start with a graph:  
\ 

```{r echo = FALSE, message=FALSE, warning=FALSE, fig.width=4, fig.height=3,out.width="6cm", fig.align='center'}
ggplot(beetle, aes(x = Dose, y = Mortality_rate)) + theme_bw() +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
geom_smooth(span = 1, colour = "red", se = FALSE) +
xlab("Dose") + ylab("Mortality rate")
```

with linear (blue) and smoothed line (red).
\textbf{Question:} (How) does the dose of the insecticide (${x}$) affect the survival probability (${y}$) of the beetles?

## 

\textbf{What can we see from the plot?}
\begin{itemize}
\item Mortality increases with higher doses of the herbicide (not surprising, right?).\\[2mm]
\item The linear line seems unreasonable. In particular, extrapolation to lower or higher doses leads to mortalities $<0$ or $>1$, which is not possible.  
\tiny (Remember: A probability is between 0 and 1 by definition.)\\[8mm]
\end{itemize}

\normalsize

\textbf{How does one analyze these data correctly?}
\begin{itemize}
\item So far, we know linear and Poisson regression.\\[2mm]
\item Both of these are \alert{not} the correct approaches here.\\[2mm]
\end{itemize}

## The "wrong" analyses

\textbf{Wrong analysis 1: Linear regression}  

We could simply use 
$$E(y_i) = \beta_0 + \beta_1 Dose_i $$
with $E(y_i)=\pi_i =$ probability to die for individuals $i$ with $Dose_i$.  
R does this analysis without complaint (!):
\small  
\ 

```{r echo = TRUE, eval = FALSE}
lm(Mortality_rate ~ Dose, data=beetle)
```

```{r echo = FALSE, eval = TRUE, message=FALSE, warning=FALSE}
r.lm <- lm(Mortality_rate ~ Dose, data=beetle)
```
\normalsize
Estimates are $\hat\beta_0 = `r format(r.lm$coef[1],2,2,2)`$ and $\hat\beta_1=`r format(r.lm$coef[2],2,2,2)`$. This means for instance that, for a zero dose, the probability to die would be $E(y_i)=`r format(r.lm$coef[1],2,2,2)`$.  
\ 
\colorbox{lightgray}{\begin{minipage}{14cm}
{\bf Problems:} 
\begin{itemize}
\item Linear regression leads to impossible predicted probability values! \\
$\Rightarrow$ \alert{Impossible predictions!}
\item For $y_i = \beta_0 + \beta_1 Dose_i + \epsilon_i$, residuals $\epsilon_i$ are {\bf not} normally distributed!
\end{itemize}
\end{minipage}}

## 

\textbf{Wrong analysis 2: Poisson regression}

What about Poisson regression with the counts `Number_killed` in the response? We could use
$$\log(E(y_i)) = \beta_0 + \beta_1 Dose_i$$

with $E(y_i)=$ number killed. Again, R does this analysis without complaining, although these are not `real' counts:
\  

\small
```{r echo=TRUE, eval=FALSE}
glm(Number_killed ~ Dose, data=beetle,family=poisson)
```

```{r echo = TRUE, eval = TRUE, message=FALSE, warning=FALSE}
r.pois <- glm(Number_killed ~ Dose, data=beetle,family=poisson)
```


\normalsize
This leads to $\hat\beta_0=`r format(r.pois$coef[1],2,2,2)`$ and $\hat\beta_1=`r format(r.pois$coef[2],2,2,2)`$. 

\colorbox{lightgray}{\begin{minipage}{14cm}
{\bf Problem:} This means for instance that, for a dose of 76, one expects that $E(y_i)=\exp(\hat\beta_0 + \hat\beta_1 \cdot 76) = 
`r format(exp(r.pois$coef[1] + r.pois$coef[2]*76),2,2,2)`$ beetles die. However, there are only 60 beetles in each group, so the predicted number killed is more than what is available. $\Rightarrow$ \alert{Impossible predictions!}
\end{minipage}}

## Sidenote: count vs.\ binomial data

Clarification of the difference between count data and binomial data:  

\textbf{Count data:} 
\begin{itemize}
\item Theoretically no upper limit on number of times an "event" (e.g., number of birds observed in a forest plot)
\item Counts cannot be expressed as a proportion.\\[6mm]
\end{itemize}  
\  
\textbf{Binomial data:}
\begin{itemize} 
\item Aggregated version of many binary experiments, that is, each can be 0 or 1. 
\item Therefore, there is an upper limit on the number of times an "event" can be observed (e.g., number of deaths cannot be greater than total number of individuals).
\item Successes can be expressed as a proportion (number of successes/number of trials).
\end{itemize}

## A model for binary data?

Remember the Bernoulli distribution from Mat183:  

\colorbox{lightgray}{\begin{minipage}{14cm}
The probability distribution of a binary random variable $Y$ $\in \{0,1\}$ with parameter $\pi$ is defined as
\begin{equation*} 
P(Y=1) = \pi \ , \quad  P(Y=0) = 1-\pi \ .
\end{equation*}
\end{minipage}}  
\  

\textbf{Characteristics of the Beroulli distribution:} \label{sl:bernoulli}
\begin{itemize}
\item $E(Y) = \pi =P(Y=1)$ \tiny (useful to remember)\\[2mm]\normalsize
\item $Var(Y)=\pi(1-\pi)$.\\[4mm]

$\rightarrow$ The variance of the distribution is determined by its mean.\\[4mm]
\end{itemize}

## From binary to binomial data

Binomial data is an \alert{aggregation of binary data}:
\begin{itemize}
\item Repeat the experiment with $P(Y=1)=\pi$ a total number of $n$ times, calculate how often a success was observed ($k$ times).

\item The expected proportion of successes (``success rate'', here $k/n$) has then the same expectation as the success probability of a single experiment: 
\begin{equation*}
E\left(\frac{\sum_{i=1}^n{Y}}{n}\right) = \pi = E(Y)  \ .
\end{equation*}
%and the variance of this $Var\left(\frac{\sum_{i=1}^n{Y}}{n}\right) = \frac{\pi(1-\pi)}{n}$.\\[4mm]
\end{itemize}

\vspace{8mm}
\small 
Example: In the beetle data  $n=49$ beetles were tested for the lowest dose, of which $k=6$ died, thus the "success rate" is $6/49=0.122$.

## The binomial distribution

The \textbf{binomial distribution} assigns the probability of seeing $k$ successes out of $n$ trials, where the success probability of a single trial is $\pi$.

\colorbox{lightgray}{\begin{minipage}{14cm}
\begin{equation*}
P(Y = k) = {{n}\choose{k}}\pi^k (1-\pi)^{n-k} \ ,\quad k=0, 1, 2,\ldots, n
\end{equation*}
In short: $$Y \sim  Binom(n,\pi) \ .$$
\end{minipage}}
\  

\textbf{Characteristics of the binomial distribution:} \label{sl:bernoulli}
\begin{itemize}
\item Mean: $E(Y)=n \cdot \pi$
\item Variance: $Var(Y)=n\cdot \pi(1-\pi)$\\[2mm]
\end{itemize}

$\rightarrow$ For given $n$, the variance is determined by its mean.  
\small R functions: `rbinom(), dbinom()`

## Doing it right: Logistic regression

We can again use the GLM machinery from last week! The \alert{linear predictor} is as always:

\begin{equation*}
\eta_i = \beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \ldots + \beta_p x_i^{(p)} \ .
\end{equation*}


We again need a \alert{link function} that relates the linear predictor $\eta_i$ to the expected value $E(y_i)$.  
\  

Remember we used the $\log$ link last week, but that seems a bad idea here (see slide 17).  
\  

The link function must be chosen such that the expected value $E(y_i)$ is always between 0 and 1!

## Link function: The logistic transformation

A transformation that assigns a probability ($\pi$) between 0 and 1 a value between $-\infty$ and $\infty$ is the \alert{logit-transformation}:  

\colorbox{lightgray}{\begin{minipage}{14cm}
$$g(\pi) = \log \left(\frac{\pi}{1-\pi}\right) = \log(\pi) - \log(1-\pi) \ .$$
\end{minipage}}

A graph depicts the functional form of $g(\cdot)$: 

```{r fig.width=4, fig.height=4,out.width="4.5cm", fig.align='center', echo = FALSE, message=FALSE, warning=FALSE}
x <- seq(0,1,0.001)
plot(x,log(x/(1-x)),xlab=expression(pi),ylab=expression(log(pi/(1-pi))),cex=0.7)
```

\small See also Box 9.2 (p.\ 123) in \emph{The new statistics with R}.

## The logistic regression model

\colorbox{lightgray}{\begin{minipage}{14cm}
In order to prevent the expected value $E(y_i)$ of a binary experiment (0/1) to attain unreasonable values, we thus formulate the \alert{logistic regression model} as
\begin{equation*}
\log\left( \frac{\pi_i}{1-\pi_i} \right) = \beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \ldots + \beta_p x_i^{(p)} \ 
\end{equation*}

with $\pi_i = P(y_i=1) \ .$
\end{minipage}}

\  

\begin{itemize}
\item The \alert{link function} is called the {\bf logistic link}.\\[2mm]
\item The \alert{family} is {\bf binomial}.\\[2mm]
\end{itemize}

## Doing it right: Fitting a logistic regression

\begin{itemize}
\item As for the Poisson GLM, we can estimate the parameters $\beta_0, \beta_1, \ldots $ by maximizing the likelihood (ML estimation).\\[3mm]

\item Luckily, the \texttt{glm()} function in R can also handle binomial and binary data!\\[3mm]

\item For \texttt{glm(..., family=binomial)}, the default link function is the logistic link. \\[3mm]

\item A complication comes from the fact that we need to tell the function \alert{two numbers for the response}:
  \begin{itemize}
  \item The number of successes, encoded as 1 (here: number killed )
  \item The number of failures, encoded as 0 (here: number survived)
  \end{itemize}
  \vspace{4mm}

\end{itemize}

\small
```{r echo = TRUE, message=FALSE, warning=FALSE}
beetle$Number_survived <- beetle$Number_tested - beetle$Number_killed
beetle.glm <- glm(cbind(Number_killed,Number_survived) ~ Dose, 
             data = beetle, family = binomial)
```

## Doing it right: Model diagnostics

As always, before looking at the regression output, let's do some model diagnostics:

```{r echo = FALSE, warning=FALSE, message=FALSE,fig.width=5, fig.height=5,out.width="6cm", fig.align='center'}
library(ggfortify)
autoplot(beetle.glm,smooth.colour=NA)
```

$\rightarrow$ Hard to see much due to very low number of data points.

## 

\begin{itemize}
\item As in Poisson regression, it is not clear how do define residuals, there are many ways (data scale, linear predictor scale, likelihood scale).\\[3mm]
\item Again, different types of residuals are used in the plots, but \texttt{autoplot()} does it automatically right. \\[3mm]
\item \alert{Be careful}: such plots are only reasonable for \alert{aggregated data} (which we have here)! The larger the groups, the more precise are the underlying assumptions (approximate equality of distributions).\\[3mm]
\item See example on slide 41 for an example with non-aggregated (binary) data.
\end{itemize}

## Doing it right: Interpreting the coefficients

Let's look at the coefficients:

\tiny
```{r echo = TRUE, message=FALSE, warning=FALSE}
summary(beetle.glm)$coef
```
\normalsize

The intercept and slope are estimated as  
\begin{equation*}
\hat\beta_0 = `r format(summary(beetle.glm)$coef[1,1],3,3,3)` \quad \text{and}\quad \hat\beta_1 = `r format(summary(beetle.glm)$coef[2,1],3,3,3)` \ ,
\end{equation*}

with standard errors and $p$-values. Very clearly, the dose influences the survival probability ($p<<0.001$), and $\hat\beta_1 >0$, thus, \alert{the larger the dose, the larger the mortality probability} (positive relation; \scriptsize be careful, this is wrong in the Hector book!!). 
\  

\normalsize
This is a \textbf{qualitative interpretation} of the coefficients.

\small 
Note: The $\beta$ coefficients are approximately normally distributed as $N(\hat\beta,\hat\sigma^2_\beta)$. 

$\rightarrow$ confidence intervals etc.\ can be calculated as in the linear case!

## Quantitative interpretation of the coefficients

Remember the regression model
\begin{equation}\label{eq:logmodel}
\log\left( \frac{\pi_i}{1-\pi_i} \right) = \beta_0 + \beta_1 Dose_i \ . 
\end{equation}


To understand what $\beta_1$ tells us, let's rearrange the equation. Solving the equation for $\pi_i$ leads to

\begin{equation}\label{eq:prob}
\pi_i = P(y_i=1 | Dose_i)= \frac{\exp(\beta_0 + \beta_1 Dose_i)}{1 + \exp(\beta_0 + \beta_1 Dose_i)} \ . 
\end{equation}
\  

\colorbox{lightgray}{\begin{minipage}{14cm}
From model \eqref{eq:logmodel} is possible to calculate the \alert{odds} ("Chance"):
\begin{equation*}
 odds(y_i=1 | Dose_i) = \frac{\pi_i}{1-\pi_i} = \frac{P(y_i=1 | Dose_i)}{P(y_i=0 | Dose_i)}= \exp(\beta_0 + \beta_1 Dose_i)\ . 
\end{equation*}
\end{minipage}}

## 

If the $Dose_i$ is then increased by 1 unit in concentration (from $x$ to $x+1$), the \alert{odds ratio} is given as

\begin{equation*}
\frac{odds(y_i=1 | Dose_i = x + 1)}{odds(y_i=1 | Dose_i = x)} = \exp{(\beta_1)} = \exp(`r format(summary(beetle.glm)$coef[2,1],3,3,3)`) = `r format(exp(summary(beetle.glm)$coef[2,1]),2,2,2)` \ .
\end{equation*}
\  

\colorbox{lightgray}{\begin{minipage}{14cm}
\textbf{Interpretation:} When the dose is increased by 1 unit, the odds to die increases by a factor of $`r format(exp(summary(beetle.glm)$coef[2,1]),2,2,2)`$.
\end{minipage}}
\  

\colorbox{lightgray}{\begin{minipage}{14cm}
Moreover, taking the $\log$ on the above equation shows that \textbf{$\beta_1$ can be interpreted as a log odds ratio}:
\begin{equation*}
\beta_1 = \log\left( \frac{odds(y_i=1 | Dose_i = x + 1)}{odds(y_i=1 | Dose_i = x)}\right)
\end{equation*}
\end{minipage}}

## Doing it right: The `anova()` table

We can look at the \alert{Analysis of Deviance} table (directly using `test="Chisq"`):

\tiny
```{r echo = TRUE, message=FALSE, warning=FALSE}
anova(beetle.glm,test="Chisq")
```
\normalsize

\textbf{Interpretation:} The total deviance is 267.66, and of this 259.23 is explained by `Dose` (using 1 degree of freedom). This seems really good, because the $\chi^2$ test gives a $p$-value that is reeeeallly small ($<2.2e-16$).

## Plotting the fit

A fitted curve can be added to the raw data by plotting $P(y_i=1)$ against the Dose, using equation (\ref{eq:prob}):

```{r echo = FALSE, warning=FALSE, message=FALSE, fig.align='center',fig.width=4, fig.height=3,out.width="6cm"}

xx <- seq(45,80,0.01)
eta <- -14.578 + 0.2455*xx
ppi <- exp(eta)/(1+exp(eta))
dd <- data.frame(xx=xx,ppi=ppi)
ggplot(beetle, aes(x = Dose, y = Mortality_rate)) + theme_bw() +
geom_point() +
xlab("Dose") + ylab("Mortality rate (Pr(Y=1))")  +
geom_line(aes(xx, ppi,colour = "red"), dd, size=1.2) + 
  scale_color_discrete(guide = FALSE)
```

(Compare to Figure 9.1 in the Hector book \emph{The new statistics with R}.)

## Overdispersion

Remember :
\begin{itemize}
\item Slides 20 and 21: $E(Y)=\pi$ and $Var(Y)=\pi(1-\pi)$, thus \alert{the variance is determined by the mean!}  


\item "Overdispersion" means \alert{"extra variability"} (larger than the model predicts or allows). \\[2mm]
\item Probable reason: Variables are missing in the model!\\[2mm]
\item Overdispersion leads to \alert{too small $p$-values}.\\[2mm]
\item Detectable by looking at the \alert{residual deviance}: \\[2mm]
\texttt{Residual deviance} $>>$ \texttt{df} $\quad\rightarrow$ Overdispersion \\[2mm]

\item Also possible: underdispersion (dependency in the data), if:\\[2mm]
\texttt{Residual deviance} $<<$ \texttt{df}  
\end{itemize}

## 

Here, the residual deviance is \textbf{8.44} with \textbf{6} degrees of freedom. Is this good or bad?  
\  

```{r echo = TRUE}
pchisq(8.438,6,lower.tail=F)
```

$\rightarrow$ $p=`r format(1-pchisq(8.438,6),2,2,2)`$ seems not problematic.  
\  

One can nevertheless account for overdispersion by switching to a \alert{`quasibinomial`} model. This allows to estimate the dispersion parameter separately.

## 

\tiny
```{r echo = TRUE, message=FALSE, warning=FALSE}
beetle.glm2 <- glm(cbind(Number_killed,Number_survived) ~ Dose, 
             data = beetle, family = quasibinomial)
summary(beetle.glm2)
```
\normalsize

## Binary response / non-aggregated data

\begin{itemize}
\item In the beetle example, we were in a comfortable situation: For each level of the does, we had several beetles. For instance, 49 beetles at lowest dose (49.06), of which 6 died (1) and 43 survived (0). This was \alert{binomial} data, an aggregated version of many (here 49) trials with 0 or 1 outcome.\\[3mm]
\item In reality, one often has only one trial (0/1) for a (combination of) covariate(s).\\[3mm]
\item The analysis is the same as for aggregated data, however there are a few complications with graphical descriptions and model checking. \\[6mm]
\end{itemize}

\textbf{Example}: Blood screening \tiny (see week 1; data from Hothorn \& Everitt 2014, chapter 7.3)

## Blood screening example

\tiny
```{r echo = FALSE, message=FALSE, warning=FALSE}
library(HSAUR3)
data("plasma",package="HSAUR3")
plasma$y <- as.integer(plasma$ESR)-1
```

```{r echo = FALSE, results='asis', warning=FALSE, message=FALSE}
library(xtable)
pol1 <- plasma[1:32,]
ttab1 <- xtable(pol1,display=c("s","f","d","s","d"))
print(ttab1, floating = TRUE, include.rownames=FALSE)
```

## 

\textbf{Question:} Is a high ESR (erythrocyte sedimentation rate) an indicator for certain diseases (rheumatic disease, chronic inflammations)?
\  

\textbf{Specifically: } Do high concentrations of the plasma proteins Fibrinogen and Globulin (which are disease indicators) increase the probability that an individual is sick (ESR$>20mm/hr$, encoded as $y_i=1$)?  
\ 

\textbf{The model to be fitted:}

\begin{equation*}
\log\left( \frac{\pi_i}{1-\pi_i} \right) = \beta_0 + \beta_1\cdot fibrinogen_i + \beta_2 \cdot globulin_i \ ,
\end{equation*}

with $E(y_i)=P(y_i=1)=\pi_i$. \\
\tiny Equivalently: $y_i \sim Bern(\pi_i)$

## Complication 1 with binary data: Graphical description

Plotting the response $y$ ($y=1$ if ESR$>20$ and $y=0$ otherwise) against the covariats does not lead to very illustrative graphs: 
\  

```{r echo = FALSE, message=FALSE, warning=FALSE, fig.align='center',fig.width=5.5, fig.height=2.5,out.width="8cm"}
require(cowplot)
theme_set(theme_cowplot(font_size=10))

p1 <- ggplot(plasma, aes(x = fibrinogen, y = y)) + theme_bw() +
geom_point() +
xlab("Fibrinogen") + ylab("y") + scale_y_continuous(breaks=c(0,1))

p2 <- ggplot(plasma, aes(x = globulin, y = y)) + theme_bw() +
geom_point() +
xlab("Globulin") + ylab("y") + scale_y_continuous(breaks=c(0,1))

plot_grid(p1, p2, ncol=2)
```

##

It is a bit more illustrative to give a \alert{conditional density plot} (`cdplot()`):
\  

```{r fig.align='center', message=FALSE, warning=FALSE, echo = FALSE, fig.width=7, fig.height=4.5,out.width="10cm"}
par(mfrow=c(1,2))
plasma$y <- as.factor(plasma$y)
cdplot(y ~ fibrinogen,plasma)
cdplot(y ~ globulin,plasma)
```

## Complication 2: Model diagnostics

\textbf{a) Residual plots:}

\alert{Plotting the residuals} is possible, but \alert{not meaningful}.
\scriptsize Why? Becasue the model checking assumptions rely on aggregated data!

```{r echo = FALSE, warning=FALSE, message=FALSE, fig.align='center',fig.width=5, fig.height=4.5,out.width="6.5cm"}
plasma.glm <- glm(y ~ fibrinogen + globulin, data = plasma, family=binomial)
autoplot(plasma.glm)
```

## 

\textbf{b) Residual deviance:}

For non-aggregated data, the `residual deviance` vs.\ `df` relation \textbf{cannot be used to detect overdispersion}!!  
\  

\scriptsize 
Why? Becasue for a single binary (0/1) variable it is impossible to estimate a variance, thus it is also impossible to say if the variance is too high/too low.

## Your turn!

Apart from the above complications, fitting and interpreting the model is analogous to aggregated binary data. Let's continue with the blood screening example:  
\  
\tiny
```{r echo = TRUE, eval = FALSE}
plasma.glm <- glm(y ~ fibrinogen + globulin, data = plasma, family = binomial)

```
\normalsize

Please look at the model outcomes (summary and anova table) on the next slides and answer the following questions:

\begin{enumerate}
\item Is there evidence for an an effect of fibrinogen and/or globulin on the outcome (ESR$>20$)?\\[2mm]
\item What is the \emph{quantitative} interpretation of the $\beta_1$ coefficient (what happens to $P(ESR>20)$ when fibrinogen incrases by 1 unit)? \\[2mm]
\item Is a \texttt{quasibinomial} model more suitable for these data?
\end{enumerate}

Please answer here: \url{http://www.klicker.uzh.ch/bkx}

## 

\tiny
```{r echo = TRUE}
summary(plasma.glm)
```

## 

\tiny
```{r echo = TRUE}
anova(plasma.glm,test="Chisq")
```

## Summary

\begin{itemize}
\item Logistic regression is useful to model binary/binomial data.\\[3mm]
\item The link function is the logistic link.\\[3mm]
\item The coefficients of logistic regression are log odds ratios \\
\begin{center}$\Leftrightarrow \quad$ $\exp(\beta)$ is an odds ratio \\[2mm]\end{center}
\item Differences between aggregated (binomial) and binary data.\\[3mm]
\item Overdispersion not detectable for binary data!
\end{itemize}