---
title: "Kurs Bio144: Datenanalyse in der Biologie"
subtitle: "Lecture 7: ANCOVA, short introduction to Linear Algebra"
author: "Stefanie Muff (Lecture) & Owen L.Petchey (Practical)"
institute: "University of Zurich"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  beamer_presentation:
    includes:
      in_header: ../../beamer_stuff/preamble.tex
classoption: "aspectratio=169"
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(xtable.comment = FALSE)
```


## Overview

\begin{itemize}
\item ANCOVA
\item Introduction to linear Algebra
\end{itemize}
\ 
Note: 
ANCOVA = ANalysi of COVAriance (Kovarianzanalyse)

## Course material covered today

\begin{itemize}
\item "Getting Started with R" chapter 6.3
\item  "Lineare regression" chapters 3.A (p.\ 43-45) and 3.4, 3.5 (p.\ 39-42)
\end{itemize}

## Recap of ANOVA

\begin{itemize}
\item ANOVA is a method to test if the means of \alert{two or more groups are different}.\\[2mm]
\item Post-hoc tests and contrasts, including correction for $p$-values, to understand the differences between the groups. \\[2mm]
\item Two-way ANOVA for factorial designs, interactions.\\[2mm]
\item ANOVA is a special case of linear regression with categorical covariates.
\end{itemize}

## Recap of two-way ANOVA example

Remember: Influence of four levels of fertilizer (DUENGER) on the yield (ERTRAG) on 5 species (SORTE) of crops was investigated. For each DUENGER $\times$ ERTRAG combination, 3 repeats were taken.

```{r message=FALSE, warning=FALSE, echo = FALSE, eval=TRUE}
library(dplyr)
path <- "../../../5_some_examples/data_examples/WBL/"
d.duenger <- read.table(paste(path,"duenger.dat",sep=""),header=T,sep=",")
d.duenger <- mutate(d.duenger,SORTE=as.factor(SORTE),DUENGER=as.factor(DUENGER))
```

Interaction plot with ERTRAG and log(ERTRAG) as response:
```{r fig.width=4,fig.height=4,out.width="4.5cm", echo=FALSE, warning=FALSE,message=FALSE}
require(cowplot)
a = interaction.plot(d.duenger$DUENGER,d.duenger$SORTE,(d.duenger$ERTRAG),xlab="Duenger",ylab="Mean Ertrag",trace.label="Sorte")

b = interaction.plot(d.duenger$DUENGER,d.duenger$SORTE,log(d.duenger$ERTRAG),xlab="Duenger",ylab="log(Mean Ertrag)",trace.label="Sorte")
plot_grid(a,b)
```

Remember: We used log(ERTRAG), because the residual plots were otherwise not ok.


## 

\tiny
```{r echo = TRUE}
r.duenger2 <- lm(log(ERTRAG) ~ DUENGER*SORTE,d.duenger)
anova(r.duenger2)
```
\ 
\normalsize
Questions: 
\begin{itemize}
\item Number of parameters? 
\item Degrees of freedom (60 data points)?
\item Interpretation?
\end{itemize}

## 
\tiny
```{r echo = TRUE, size = 'tiny'}
summary(r.duenger2)
```

## Analysis of Covariance (ANCOVA)

\colorbox{lightgray}{\begin{minipage}{14cm}
An ANCOVA is an analysis of variance (ANOVA), \alert{including} also at least one \alert{continuous covariate}.
\end{minipage}}
\vspace{6mm}

ANCOVA unifies several concepts that we approached in this course so far:

\begin{itemize}
\item Linear regression\\[3mm]
\item Categorical covariates\\[3mm]
\item Interactions (of continuous and categorical covariates)\\[3mm]
\item Analysis of Variance (ANOVA)\\[6mm]
\end{itemize}

As such, it is a __special case of the linear regression model__.

## 

Given a categorical covariate $x_i$ and a continuous covariate $z_i$. Then the ANCOVA equation (without interactions) is given as

\colorbox{lightgray}{\begin{minipage}{14cm}
\begin{equation*}
y_i = \beta_0 + \beta_1x_i^{(1)} + ... + \beta_kx_i^{(k)} + \beta_z z_i + \epsilon_i \ ,
\end{equation*}
where $x_i^{(k)}$ is the $k$th dummy variable ($x_i^{(k)}$=1 if $i$th observation belongs to category $k$, 0 otherwise).
\end{minipage}}
\ 


__Note 1:__ It is straightforward to add an interaction of $x_i$ with $z_i$.
\  

__Note 2:__ Again, for identifiability reason, we typically set $\beta_1=0$.

## Once more: the earthworms

"Magenumfang" was used to predict "Gewicht" of the worm, including as covariate also the worm species.

```{r echo = FALSE, eval = TRUE}
d.wurm <- read.table ("../../../5_some_examples/data_examples/ancova/Projekt6Regenwuermer/RegenwuermerDaten_Haupt.txt",header=T)
d.wurm[d.wurm$Gattung=="N","GEWICHT"] <- d.wurm[d.wurm$Gattung=="N","GEWICHT"]*0.5
```

```{r fig.width=5,fig.height=3.5,out.width="6.5cm", echo=FALSE, message=FALSE,warning=FALSE,fig.align='center'}
require(ggplot2)
require(dplyr)
ggplot(d.wurm,aes(x=MAGENUMF,y=log10(GEWICHT),colour=Gattung)) + geom_point() + theme_bw()
```

\alert{Categorical} and \alert{continuous} covariates were used to predict a continuous outcome $\rightarrow$ ANCOVA.

## 
\tiny
```{r echo = TRUE, eval = TRUE}
r.lm <- lm(log(GEWICHT) ~  MAGENUMF + Gattung,d.wurm)
summary(r.lm)$coef
```
\normalsize  
\ 

__Important:__  The $p$-values for the entries `GattungN` and `GattungOc` are not very meaningful (why?).

To understand if "Gattung" has an effect, __we need to carry out an $F$-test__ $\rightarrow$ ANOVA table:
\tiny  

\  

```{r echo = TRUE, eval = TRUE}
anova(r.lm)
```

## 

We also included an __interaction term__ between `MAGENUMF` and `Gattung` to allow for different slopes:  

\ 

```{r fig.width=5, fig.height=3.5,out.width="6.5cm", fig.align='center', echo=FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
library(dplyr)
ggplot(d.wurm,aes(x=MAGENUMF,y=log10(GEWICHT),colour=Gattung)) + geom_point() + geom_smooth(method="lm") + theme_bw()
```

## 

$\rightarrow$ We again need the __$F$-test__ to check whether the respective interaction term is needed:  

\ 

\small
```{r echo = TRUE, eval = TRUE}
r.lm2<- lm(log(GEWICHT) ~  MAGENUMF * Gattung,d.wurm)
anova(r.lm2)
```
\normalsize
$\rightarrow$ $p=0.167$, thus interaction is probably not relevant. 

## A new example: cholesterol levels

__Example:__ Cholesterol levels [mg/ml] for 45 women from three US states (Iowa, Nebraska, Arizona), were measured.

__Question:__ Do these levels differ between the states?

Age (years) may be a relevant covariable.

```{r echo = FALSE}
d.chol <- read.table ("../../../5_some_examples/data_examples/ancova/cholesterol/cholesterol.txt",header=T)
 set.seed(23)
 age <- as.integer(runif(15,19,65))
 dd2 <- data.frame(cholesterol=round(rnorm(15,10 + 4.5*age,35),0),age=age,state=rep("Arizona",15))
 d.chol <- rbind(d.chol,dd2)
```


```{r fig.width=4, fig.height=3.5,out.width="5.7cm",fig.align='center',warning=FALSE,message=FALSE, echo = FALSE}
library(ggplot2)
ggplot(d.chol,aes(x=state,y=cholesterol)) + geom_boxplot()
```

## 

The scatter plot gives an idea about the model that might be useful here:

```{r, message=FALSE, warning=FALSE, echo = FALSE, fig.align='center',fig.width=5, fig.height=3.5,out.width="6.5cm"}
library(ggplot2)
library(dplyr)
ggplot(d.chol,aes(x=age,y=cholesterol,colour=state)) + geom_point() + geom_smooth(method="lm") + theme_bw()
```

$\rightarrow$ We include state, age and the interaction of the two. 

## 

Doing the analysis:

```{r echo = TRUE}
r.lm <- lm(cholesterol ~ age*state,data=d.chol)
anova(r.lm)
```

Interpretation?

## 

Compare the results from the previous slide to the estimated coefficients:

```{r echo = TRUE}
r.lm <- lm(cholesterol ~ age*state,data=d.chol)
summary(r.lm)$coef
```

__Note:__ The $p$-values for the `age` coefficient is not the same as in the ANOVA table.
__Reason:__ `anova()` tests the models against one another in the __order__ specified.

## 

As always, some model checking is necessary:  

\ 
```{r fig.width=6, fig.height=3.5,out.width="7.5cm", fig.align='center', echo = FALSE, message=FALSE, warning=FALSE}
require(ggfortify)
autoplot(r.lm,which=c(1,2),smooth.colour=NA)
```

\ 

$\rightarrow$ This seems ok.

## An introduction to linear Algebra

Who has some knowledge of linear Algebra?

__Overview__ 

\begin{itemize}

\item The basics about  
\begin{itemize}
\item vectors \\[1mm]
\item matrices \\[1mm]
\item matrix algebra \\[1mm]
\item matrix multiplication \\[2mm]
\end{itemize}
\item Why is linear Algebra useful? \\[2mm]
\item What does it have to do with data analysis and statistics?\\[2mm]
\item Regression equations in matrix notation.
\end{itemize}

## Motivation

Why are vectors, matrices and their algebraic rules useful?

\begin{itemize}
\item \textbf{Example 1:} The observations for a covariate ${x}$ or the response ${y}$ for all individuals $1\leq i\leq n$ can be stored in a vector (vectors and matrices are always given in \textbf{bold} letters):
\begin{equation*}
{x}=\left(
\begin{array}{c}
x_1 \\
x_2 \\
... \\
x_n
\end{array}
\right) \ , \quad
{y}=\left(
\begin{array}{c}
y_1 \\
y_2 \\
... \\
y_n
\end{array}
\right)  \ .
\end{equation*}
~\\[2mm]
\item \textbf{Example 2:} Covariance matrices for multiple variables. Say we have ${x}^{(1)}$ and ${x}^{(2)}$. The \alert{covariance matrix} is then given as
\begin{equation*}
\left(
\begin{array}{cc}
Var({x}^{(1)}) & Cov({x}^{(1)},{x}^{(2)}) \\
Cov({x}^{(1)},{x}^{(2)}) & Var({x}^{(2)}) \\
\end{array}
\right) \ .
\end{equation*}
~\\
\end{itemize}

## 

\begin{itemize}
\item \textbf{Example 3:} The \alert{data} (e.g.\ of some regression model) can be stored in a \alert{matrix}:
\begin{equation*}
{\tilde{X}} = 
\left(
\begin{array}{ccc}
1 & x_1^{(1)} &  x_1^{(2)}\\
1 & x_2^{(1)} & x_2^{(2)} \\
... & ... &... \\
1 & x_n^{(1)} & x_n^{(2)}
\end{array}
\right) \ .
\end{equation*}
This is the so-called \alert{design matrix} with a vector of 1's in the first column.~\\[4mm]

\item \textbf{Example 4:} A linear regression model can be written compactly using \alert{matrix multiplication}:
%
\begin{equation*}
{y} = {\tilde{X}} \cdot {\tilde\beta} + {e}  \ ,
\end{equation*}

with ${\tilde\beta}$ the vector of regression coefficients and ${e}$ the vector of errors
\end{itemize}


## 

Why do we discuss this topic in our course?

\begin{itemize}
\item Useful for \alert{compact notation}.\\[2mm]
\item Enabels you to \alert{understand many statistical texts} (books, research articles) that remain inaccessible otherwise. \\[2mm]
\item Useful for \alert{efficient coding}, e.g. in R, which helps to increase speed and to reduce error rates.\\[2mm]
\item More advanced concepts often rely on linear algebra, e.g. \alert{principal component analysis} (PCA) or \alert{random effects} models.\\[2mm]
\item Is part of a \alert{general education} (Allgemeinbildung) ;-) \\
\end{itemize}

## Marices

An $n\times m$ \alert{Matrix} is given as
\begin{equation*}
{A} = \left(
\begin{array}{cccc}
a_{11} & a_{12} &  ... & a_{1m}\\
a_{21} & a_{22} & ... & a_{2m} \\
\vdots & \vdots &  & \vdots \\
a_{n1} & a_{n2} & ... & a_{nm}
\end{array}\right) \ ,
\end{equation*}
with rows $1 = 1, \ldots , n$ and columns $j=1,\ldots,m$.

\textbf{Quadratic matrix:} $n=m$. Example:
\begin{equation*}
\left(
\begin{array}{ccc}
1 & 2 & 3 \\
4 & 3 & 2 \\
6 & 1 & 9 \\
\end{array}
\right)
\end{equation*}

## 

\textbf{Symmetric matrix:} $a_{ij} = a_{ji}$. Example:
\begin{equation*}
\left(
\begin{array}{ccc}
1 & 2 & 3 \\
2 & 3 & 4 \\
3 & 4 & 5 \\
\end{array}
\right) \ .
\end{equation*}


\textbf{The diagonal of a quadratic matrix} is given by $(a_{11},a_{22},\ldots , a_{nn})$. Example: the diagonal of the above matrix is given as
\begin{equation*}
(a_{11},a_{22},a_{33}) = (1,3,5) \ .
\end{equation*}


\textbf{Diagonal matrix:} A matrix that has entries $\neq 0$ \alert{only on the diagonal}. Example:
\begin{equation*}
\left(
\begin{array}{ccc}
1 & 0 & 0 \\
0 & 3 & 0 \\
0 & 0 & 5 \\
\end{array}
\right) \ .
\end{equation*}

## 

\textbf{Transposing a matrix:} Given a matrix ${A}$. Exchange the rows by the columns and vice versa. This leads to the \alert{transposed matrix} ${A}^\top$:
\begin{equation*}
{A} = \left(
\begin{array}{cccc}
a_{11} & a_{12} &  ... & a_{1m}\\
a_{21} & a_{22} & ... & a_{2m} \\
\vdots & \vdots &  & \vdots \\
a_{n1} & a_{n2} & ... & a_{nm}
\end{array}\right) \quad \Rightarrow \quad
{A}^\top = \left(
\begin{array}{cccc}
a_{11} & a_{21} &  ... & a_{n1}\\
a_{12} & a_{22} & ... & a_{n2} \\
\vdots & \vdots &  & \vdots \\
a_{1m} & a_{2m} & ... & a_{nm}
\end{array}\right) \ .
\end{equation*}


Examples (note also the change of dimensions):
\begin{equation*}
{A} = \left(
\begin{array}{cccc}
1 & 2 & 3 & 4\\
5 & 6 & 7 & 8 \\
9 & 10  & 11  & 12 \\
13 & 14 & 15 & 16 \\
\end{array}\right) \quad \Rightarrow \quad 
{A}^\top = \left(
\begin{array}{cccc}
1 & 5 & 9 & 13\\
2 & 6 & 10 & 14 \\
3 & 7  & 11  & 15 \\
4 & 8 & 12 & 16\\
\end{array}\right)
\end{equation*}

\begin{equation*}
{A} = \left(
\begin{array}{cccc}
1 & 2 & 3 & 4\\
5 & 6 & 7 & 8 \\
\end{array}\right) \quad \Rightarrow \quad 
{A}^\top = \left(
\begin{array}{cc}
1 & 5  \\
2 & 6   \\
3 & 7  \\
4 & 8 \\
\end{array}\right)
\end{equation*}

## 

\begin{itemize}
\item Transposing a matrix \alert{twice} leads to the original matrix: $$({A}^\top)^\top = {A} \ .$$ 


\item When a matrix is \alert{symmetric}, then $${A}^\top = {A} \ .$$
This is true in particular for diagonal matrices.
\end{itemize}

## Vectors

A vector is nothing else than $n$ numbers written in a column:
\begin{equation*}
{b} = \left(
\begin{array}{c}
b_1 \\ 
b_2 \\
\vdots \\
b_n
\end{array}
\right)
\end{equation*}

\textbf{Transposing} a vector leads to a \emph{row vector}:
\begin{equation*}
\left(
\begin{array}{c}
b_1 \\ 
b_2 \\
\vdots \\
b_n
\end{array}
\right)^\top
 = \left(
\begin{array}{cccc}
b_1  & b_2  & \hdots & b_n
\end{array}
\right)
\end{equation*}

\textbf{Note:} By definition (by default), a vector is always a column vector. 

## Addition and subtraction

\begin{itemize}
\item Adding and subtracting matrices and vectors is only possible when the objects have the \alert{same dimension}.
\item Examples: Elementwise addition (or subtraction)

\begin{equation*}
\left(
\begin{array}{ccc}
1 & 2 & 3 \\ 
4 & 5 & 6 \\
\end{array}
\right) 
+
\left(
\begin{array}{ccc}
3 & 2 & 1 \\ 
6 & 5 & 4 \\
\end{array}
\right) 
 = 
 \left(
\begin{array}{ccc}
4 & 4 & 4 \\ 
10 & 10 & 10 \\
\end{array}
\right)
\end{equation*}

\begin{equation*}
\left(
\begin{array}{c}
1   \\ 
4  \\
\end{array}
\right) 
-
\left(
\begin{array}{ccc}
3   \\ 
9 \\
\end{array}
\right) 
 = 
 \left(
\begin{array}{c }
-2\\ 
-5\\
\end{array}
\right)
\end{equation*}


~\\[2mm]

\item But this addition is \alert{not defined}:
\begin{equation*}
\left(
\begin{array}{ccc}
1 & 2 & 3 \\ 
4 & 5 & 6 \\
\end{array}
\right) 
+
\left(
\begin{array}{cc}
3 & 6   \\ 
2 & 5  \\
1 & 4
\end{array}
\right)  = 
\end{equation*}

\end{itemize}

## Multiplication by a scalar

Multiplication with a "number" (scalar) is simple: Multiply each element in a vector or a matrix.

Examples:


\begin{equation*}
3 \cdot \left(
\begin{array}{ccc}
1 & 2 & 3 \\ 
4 & 5 & 6 \\
\end{array}
\right) 
 = 
 \left(
\begin{array}{ccc}
3 & 6 & 9 \\ 
12 & 15 & 18 \\
\end{array}
\right)
\end{equation*}


\begin{equation*}
-2 \cdot \left(
\begin{array}{c}
1 \\ 
4 \\
-2 \\
\end{array}
\right) 
 = 
 \left(
\begin{array}{c}
-2 \\ 
-8 \\
4 \\
\end{array}
\right) 
\end{equation*}

## Matrix multiplication

\colorbox{lightgray}{\begin{minipage}{14cm}
The multiplication of two matrices ${A}$ and ${B}$ is \alert{defined if}
\begin{center}number of columns in ${A}$ = number of rows in ${B}$.\end{center}
\end{minipage}}


It is easiest to explain matrix multiplication with an example:
\begin{eqnarray*}
\left(
\begin{array}{cc}
2 & 1 \\
-1 & 0 \\
3 & 1 \\
\end{array}
\right) \cdot 
\left(
\begin{array}{cc}
3 & 1 \\
4 & -2 \\
\end{array}
\right) &=& 
\left(
\begin{array}{cc}
2\cdot 3 + 1\cdot 4   &   2\cdot 1 + 1\cdot (-2) \\
(-1)\cdot 3 + 0\cdot 4 & (-1)\cdot 1 + 0\cdot (-2) \\
3\cdot 3 + 1\cdot 4 & 3\cdot 1 + 1\cdot (-2)
\end{array}
\right) \\
&=& \left(
\begin{array}{cc}
10   &   0 \\
-3  & -1 \\
13  & 1
\end{array}
\right)
\end{eqnarray*} 

\href{http://matrix.reshish.com/multiplication.php}
{\beamergotobutton{Matrix multiplication app}}

## Matrix multiplication rules I

Attention: Matrix multiplication does \alert{not} follow the same rules as scalar multiplication!!!

\begin{itemize}
\item It can happen that ${A}\cdot {B}$ can be calculated, but ${B}\cdot {A}$ is not defined (see example on previous slide). \\[4mm]
\item In general: ${A}\cdot {B} \neq  {B}\cdot {A}$, even if both are defined.\\[4mm]
\item It can happen that ${A}\cdot {B} ={0}$ (${0}$ matrix), although both ${A}\neq {0}$ and ${B}\neq {0}$. \\[4mm]
\item The \alert{Assoziativgesetz holds:} ${A}\cdot ({B} \cdot {C}) = ({A}\cdot {B}) \cdot {C}$. \\[4mm]
\item The \alert{Distributivgesetz holds:} 
\begin{eqnarray*}
{A}\cdot ({B} + {C}) &=& {A}\cdot {B} +   {A}\cdot {C}  \\
({A} + {B} ) \cdot {C} &=& {A}\cdot {C} + {B}\cdot {C} 
\end{eqnarray*}
\end{itemize}

## Matrix multiplication rules II

\begin{itemize}
\item Transposing inverts the order: $({A}\cdot {B})^\top = {B}^\top \cdot {A}^\top$.\\[4mm]
\item The product ${A}\cdot{A}^\top$ is \alert{always symmetric}. \\[4mm]
\item All these rules also hold for \alert{vectors}, which can be interpreted as $n\times 1$ matrices:
\begin{equation*}
{a}\cdot {b}^\top = 
\left(
\begin{array}{cccc}
a_1b_1 & a_1 b_2 & \ldots & a_1 b_m \\
a_2b_1 & a_2 b_2 & \ldots & a_2 b_m \\
\vdots & \vdots & & \vdots \\
a_n b_1 & a_n b_2 & \ldots & a_n b_m \\
\end{array}
\right)
\end{equation*}
If ${a}$ and ${b}$ have the \alert{same length}:
\begin{equation*}
{a}^\top \cdot {b}  = 
 \sum_i a_i  b_i
\end{equation*}
\end{itemize}

## Short exercise

Given vectors ${a}$ and ${b}$ and matrix ${C}$:
\begin{equation*}
{a} =\left( \begin{array}{c} 1 \\  -2 \\ 3 \\ 0 \end{array}\right), \quad
{b} = \left( \begin{array}{c}-2\\  4 \end{array}\right), \quad 
{C}=\left(\begin{array}{cc} 1 & 0 \\ -1 & 2 \end{array}\right)
\end{equation*}


Calculate, if defined
\begin{itemize}
\item ${a}^\top \cdot {b}$ \\[2mm]
\item ${a} \cdot {b}^\top$ \\[2mm]
\item ${C}\cdot {a}$ \\[2mm]
\item ${C}\cdot {b}$ \\[2mm]
\end{itemize}

## The length of a vector

The \alert{length of a vector} ${a}^\top=(a_1,a_2,\ldots, a_n)$ is defined as $||{a}||$ with
\begin{equation*}
||{a}||^2 = {a}^\top \cdot {a} =   \sum_i a_i^2  \ .
\end{equation*}

This is basically the \alert{Pythagoras} idea in 2, 3, ... $n$ dimensions.

In 2 dimensions: $||{a}|| = \sqrt{a_1^2 + a_2^2}$:

<!--\begin{center}
\includegraphics[width=4cm]{graphics/pythagoras.pdf}
\end{center}
}-->

## Identity matrix (Einheitsmatrix)

The identity matrix (of dimension $m$) is probably the simplest matrix that exists. It has 1's on the diagonal and 0's everywhere else:

\begin{equation*}
{I} = 
\left( 
\begin{array}{cccc}
1 & 0 & \ldots & 0 \\
0 & 1 & \ldots & 0 \\
\vdots & \vdots && \vdots \\
0 & 0 & \ldots & 1 
\end{array}
\right)
\end{equation*}


Multiplication with the identity matrix leaves a $m\times n$ matrix ${A}$ unchanged:
$${A} \cdot {I}  = {A} \ .$$


## Inverse matrix

\colorbox{lightgray}{\begin{minipage}{14cm}
Given a quadratic matrix ${A}$ that fulfills
\begin{equation*}
{B}\cdot {A} = {I} \ ,
\end{equation*}
then ${B}$ is called the \alert{inverse} of ${A}$ (and vice versa). One then writes 
$${B}={A}^{-1} \ .$$
\end{minipage}}

Note: 
\begin{itemize}
\item In that case it also holds that ${A}\cdot {B} = {I}$.\\[2mm]
\item Therefore: $\, {A}={B}^{-1} \quad  \Leftrightarrow \quad  {B}={A}^{-1}$ \\
\end{itemize}


## 

\begin{itemize}
\item The inverse of ${A}$ may \alert{not exist}. If it exists, ${A}$ is \alert{regular}, otherwise \alert{singular}.\\[4mm]
\item $({A}^{-1})^{-1} = {A}$. \\[4mm]
\item The inverse of a matrix product is given as 
$$({A}\cdot {B})^{-1} = {B}^{-1} \cdot {A}^{-1} \ .$$\\

\item It is
$$({A}^\top)^{-1} = ({A}^{-1})^\top \ .$$
Therefore one may also write ${A}^{-\top}$.
\end{itemize}

## Linear regression in matrix notation

Linear regression with $n$ data points can be understood as an \alert{equation system with $n$ equations}.

Remember example 4 from slide 21: We said that a linear regression model can be written compactly using \alert{matrix multiplication}:

\begin{equation*}
{y} = {\tilde{X}} \cdot {\tilde\beta} + {e}  \ .
\end{equation*}


Task: Verify this now, using a model with two variables ${x}^{(1)}$ and ${x}^{(2)}$ and
\begin{equation*}
{y}=\left(\begin{array}{c} y_1 \\ y_2 \\ \vdots \\ y_n\end{array}\right), \;
\, 
{\tilde{X}} = 
\left(
\begin{array}{ccc}
1 & x_1^{(1)} &  x_1^{(2)}\\
1 & x_2^{(1)} & x_2^{(2)} \\
... & ... &... \\
1 & x_n^{(1)} & x_n^{(2)}
\end{array}
\right), \;
{\tilde\beta}=\left(\begin{array}{c} \beta_0 \\ \beta_1 \\ \beta_2\end{array}\right), \;
{e}=\left(\begin{array}{c} e_1 \\ e_2 \\ \vdots \\ e_n\end{array}\right).
\end{equation*}

## 

It can be shown (see Stahel 3.4f,g) that \alert{the least-squares estimates} $\hat{\beta}$ can be calculated as
\begin{equation*}
\hat{\beta} = (\tilde{{X}}^\top \tilde{{X}})^{-1} \cdot \tilde{{X}}^\top \cdot {y} 
\end{equation*}

Does this look complicated? 
\  

Let's test this in R ....

## Doing linear algebra in R

Let us look at model ${y} = {\tilde{X}} \cdot {\tilde\beta} + {e}$ with coefficients
\  

$\beta_0=10, \beta_1=5, \beta_2=-2$ and variables  

\ 

\begin{center}
\begin{tabular}{ccc}
$i$ & $x_i^{(1)}$ & $x_i^{(2)}$ \\
\hline
1 & 0 & 4 \\
2 & 1 & 1\\
3 & 2 & 0\\
4 & 3 &1\\
5 & 4 & 4\\
\end{tabular}
\end{center}

Thus the model is given as 
\begin{equation*}
y_i = 10 + 5 x_i^{(1)}  -2 x_i^{(2)} + \epsilon_i \ , \text{for }\quad 1\leq i \leq n \ . 
\end{equation*}
Let us start by generating the "true" response, calculated as ${\tilde{X}} {\tilde\beta}$

##

\small
```{r echo = TRUE}
x1 <- c(0,1,2,3,4)
x2 <- c(4,1,0,1,4)
Xtilde <- matrix(c(rep(1,5),x1,x2),ncol=3)
Xtilde
t.beta <- c(10,5,-2)
t.y <- Xtilde%*%t.beta
t.y
```

\normalsize
Matrix multiplication in R is done by the \alert{`\%*\%`} symbol. 


## 

Next, we generate the vector containing the $\epsilon_i \sim N(0,\sigma^2)$ with $\sigma^2=1$:

\small
```{r echo = TRUE}
t.e <- rnorm(5,0,1)
t.e
```

\normalsize

which we add to the "true" ${y}={\tilde{X}} {\tilde\beta}$ values, to obtain the "observed" values: 

\small
```{r echo = TRUE}
t.Y <- t.y  + t.e
t.Y
```
\normalsize

It is now possible to fit the model with `lm`: 

\small
```{r echo = TRUE}
r.lm <- lm(t.Y ~ x1 + x2)
summary(r.lm)$coef
```

## 

Alternatively, we can use formula

\begin{equation*}
\hat{\beta} = (\tilde{{X}}^\top \tilde{{X}})^{-1}   \tilde{{X}}^\top   {y} 
\end{equation*}

to find the parameter estimates: 

\small
```{r echo = TRUE}
solve(t(Xtilde) %*% Xtilde)  %*%  t(Xtilde) %*% t.Y
```

\normalsize

\begin{itemize}
\item \texttt{solve()} calculates the \alert{inverse} (here the inverse of $\tilde{{X}}^\top \tilde{{X}}$).
\item \texttt{t()} gives the \alert{transposed} (here of $\tilde{{X}}^\top$).\\[4mm]
\end{itemize}

__Task:__ Do this calculation by yourself and verify for each step that the dimensions of the matrices and the vector are indeed fitting, so that this expression is defined.

## 

\begin{center}
{\bf Appendix}
\end{center}

## Some R commands for matrix algebra

Reading vectors and a matrices into R:

\small

```{r echo = TRUE}
a <- c(1,2,3)
a
```

```{r echo = TRUE}
A <- matrix(c(1,2,3,4,5,6),byrow=T,nrow=2)
B <- matrix(c(6,5,4,3,2,1),byrow=T,nrow=2)
A
B
```

## 

Adding and subtracting: 

\small
```{r echo = TRUE}
A + B
A - B
```

However, be careful, R sometims does unreasonable things: 

```{r echo = TRUE}
A + a
```

What happened here??

##

Matrix multiplication: 
\tiny
```{r echo = TRUE}
C <- A %*% t(B)
C
A%*%a
```
\normalsize

Matrix inversion (possible for quadratic matrices only):

\tiny
```{r echo = TRUE}
solve(C)
C %*% solve(C)
```
\normalsize

Why does `solve(A)` or `solve(B)` not work?

