---
title: "Lecture 12: Measurement Error"
subtitle: "BIO144 Data Analysis in Biology"
author: "Owen Petchey & Stephanie Muff"
institute: "University of Zurich"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  beamer_presentation:
    includes:
      in_header: ../../../beamer_stuff/preamble.tex
classoption: "aspectratio=169"
---

```{r setup, include=FALSE, echo = FALSE, message=FALSE, warning=FALSE}
source(here::here("2_content/beamer_stuff/preamble.r"))
```

## Overview

* Measurement error (ME) in covariates ($x$) and in the response ($y$) of regression models.
* Effects of ME on regression parameters.
*  When do I have to worry?
* Simple methods to correct for ME. 

## Course material covered today

The lecture material of today is partially based on the following literature:

* Chapter 6.1 in "Lineare regression" (BC reading)


## Sources of measurement error (ME)

* **Measurement imprecision** in the field or in the lab (length, weight, blood pressure, etc.).
* Errors due to **incomplete** or **biased observations** (e.g., self-reported dietary aspects, health history).
* Biased observations due to **preferential sampling or repeated observations**.
* Rounding error, digit preference.
* **Misclassification error** (e.g., exposure or disease classification).
* \ldots

\begin{center}
\textcolor{teal}{"Error" is often used synonymous to "uncertainty".}
\end{center}







## Another fundamental assumption (often neglected!)


* It is a \alert{fundamental assumption}  that explanatory variables are measured or estimated \alert{without error}, for instance for 

* the calculation of correlations.
* linear regression and ANOVA.
* Generalized linear and non-linear regressions (e.g.\ logistic and Poisson).


* Violation of this assumption may lead to \alert{biased} parameter estimates, altered standard errors and $p$-values, incorrect covariate importances, and to \alert{misleading conclusions}.
* Even standard statistics textbooks do often not mention these problems.



\begin{center}
\textcolor{teal}{Measurement error in the covariates ($\mathbf{x}$) violates an assumption of standard regression analyses!! }
\end{center}





## Classical measurement error

A very common error type:

Let \alert{$x_i$} be the *correct but unobserved* variable and $w_i$ the observed variable with error $u_i$. Then
$$w_i = x_i + u_i, \qquad u_i \sim N(0,\sigma^2_{u})$$
is the **classical ME model**.

```{r fig.width=1.5, fig.height=1.3, fig.align='center'}
par(mar=c(0.1,0.1,0.1,0.1))
tx<-seq(-4,4,0.01)
par(mfrow=c(1,1))
plot(x = tx, dnorm(tx,0,1),type="l",xaxt="n",yaxt="n",xlab="",ylab="")
abline(v=0,lty=2,lwd=0.5)
text(0,0.02,labels=expression(x[i]),cex=0.6)
text(-1.5,0.3,labels=expression(w[i]),cex=0.6)
```
**Examples:**   Imprecise measurements of a concentration, a mass, a length etc.
$\rightarrow$ The observed value $w_i$ varies around the true value $x_i$.







## Illustration of the problem


Find regression parameters $\beta_0$ and $\beta_x$ for the model with covariate $\mathbf{x}$:


$$y_i = 1 \cdot x_i + \epsilon_i, \qquad \epsilon_i \sim N(0,\sigma^2)$$



\vspace{1mm}

```{r, message=FALSE, fig.width=3, fig.height=2, fig.align='center'}
library(ggplot2)
set.seed(84522)
col1 <- "red"
col2 <- "blue"
n <- 100
beta_0 <- 0
beta_1 <- 1
epsilon <- rnorm(n, 0, 0.2)
x <- rnorm(n, 0, 1)
u <- rnorm(n, 0, 1)
w <- x + u

# Classical
y <- beta_0 + beta_1*x + epsilon
m1 <- lm(y~x)
m2 <- lm(y~w)
#nf <- layout(matrix(c(1,2),1,2,byrow=TRUE), c(4,4), c(4,2), TRUE)
#par( mar=c(4,6,4,1), cex.lab=1.5, cex.axis=1.4, cex.main=1.8)
cols <- c("X"="black","W"="red")
ggplot(mapping=aes(x=x,y=y,col="X")) +  geom_smooth(method="lm") + geom_point(size=0.9) + 
  # geom_smooth(mapping=aes(x=w,y=y,color="W"),method="lm") +  geom_point(mapping=aes(x=w,y=y,color="W"),size=0.9) + 
  scale_colour_manual(name="Variable",values=cols) +
  xlab("X") +
  xlim(c(-3.5,3.5)) +
  ylim(c(-2.7,2.7)) +
  theme_bw()
```




## Illustration of the problem II


However, assume that only an erroneous proxy $\mathbf{w}$ is observed with classical ME


$$w_i = x_i + u_i, \qquad u_i \sim N(0,\sigma_u^2), \qquad \sigma^2_u = \sigma^2_x$$

\vspace{1mm}

```{r, message=FALSE, fig.width=3, fig.height=2, fig.align='center'}
library(ggplot2)
set.seed(84522)
col1 <- "red"
col2 <- "blue"
n <- 100
beta_0 <- 0
beta_1 <- 1
epsilon <- rnorm(n, 0, 0.2)
x <- rnorm(n, 0, 1)
u <- rnorm(n, 0, 1)
w <- x + u
##Classical
y <- beta_0 + beta_1*x + epsilon
m1 <- lm(y~x)
m2 <- lm(y~w)
#nf <- layout(matrix(c(1,2),1,2,byrow=TRUE), c(4,4), c(4,2), TRUE)
#par( mar=c(4,6,4,1), cex.lab=1.5, cex.axis=1.4, cex.main=1.8)
cols <- c("X"="black","W"="red")
ggplot(mapping=aes(x=x,y=y,color="X")) +  geom_smooth(method="lm") + geom_point(size=0.9) + 
  geom_smooth(mapping=aes(x=w,y=y,color="W"),method="lm") +  geom_point(mapping=aes(x=w,y=y,color="W"),size=0.9) + 
  scale_colour_manual(name="Variable",values=cols) +
  xlab("X or W") +
  theme_bw()
```


## A tool you can have a play with...


[\beamergotobutton{Illustration in a browser application} ](https://stefaniemuff.shinyapps.io/MEC_ChooseL/)

```{r out.width='60%', fig.asp=.75, fig.align='center', echo=FALSE}
knitr::include_graphics("images/me_browser_spp.png")
```






##  The "Triple Whammy of Measurement Error"

(Carroll et al. 2006)


1 \alert{Bias}: The inclusion of erroneous variables in downstream analyses may lead to biased parameter estimates.

2 ME leads to a \alert{loss of power} for detecting signals.

3 ME \alert{masks imporant features} of the data, making graphical model inspection difficult.


```{r, message=FALSE, fig.width=3, fig.height=2, fig.align='center'}

library(ggplot2)
set.seed(84522)
col1 <- "red"
col2 <- "blue"
n <- 100
beta_0 <- 0
beta_1 <- 1
epsilon <- rnorm(n, 0, 0.2)
x <- rnorm(n, 0, 1)
u <- rnorm(n, 0, 1)
w <- x + u
##Classical
y <- beta_0 + beta_1*x + epsilon
m1 <- lm(y~x)
m2 <- lm(y~w)
#nf <- layout(matrix(c(1,2),1,2,byrow=TRUE), c(4,4), c(4,2), TRUE)
#par( mar=c(4,6,4,1), cex.lab=1.5, cex.axis=1.4, cex.main=1.8)
cols <- c("X"="black","W"="red")
ggplot(mapping=aes(x=x,y=y,color="X")) +  geom_smooth(method="lm") + geom_point(size=0.9) + 
  #geom_smooth(mapping=aes(x=w,y=y,color="W"),method="lm") +  
  geom_point(mapping=aes(x=w,y=y,color="W"),size=0.9) + 
  scale_colour_manual(name="Variable",values=cols) +
  xlab("X or W") +
  theme_bw()

```





## How to correct for error?

* Generally, to correct for the error we need an **error model** and knowledge of the **error model parameters**
**Example** If classical error $w_i = x_i + u_i$ with $u_i \sim N(0,\sigma_u^2)$ is present, knowledge of the **error variance** $\sigma_u^2$ is needed.

**Strategy**: Take repeated measurements to estimate the error variance!

* In \alert{simple cases}, formulas for the bias exist.

* In most cases, such simple relations don't exist. Specific error modeling methods are then needed!





## Attenuation in simple linear regression

Given the simple linear regression equation $y_i = \beta_0 + \beta_x x_i + \epsilon_i$ with $w_i = x_i + u_i$. Assume that $w_i$ instead of $x_i$ is used in the regression:
$$y_i = \beta^\star_0 + \beta^\star_x w_i + \epsilon_i$$


The **naive slope parameter** $\beta_x^\star$} is then underestimated with respect to the true slope $\beta_x$, with **attenuation factor** $\lambda$: 
$$\beta_x^\star =\underbrace{ \left(\frac{\sigma_x^2}{\sigma_x^2 + \sigma_u^2}\right)}_{=\lambda} \beta_x$$
$\rightarrow$ knowing $\sigma_u^2$ and $\sigma_x^2$, the correct slope can be retrieved!

**Example:** $\sigma_x^2 = 5$, $\sigma_u^2 = 1$, $\rightarrow$  $\lambda = \frac{5}{6} = 0.83$




## Error modeling

The **two most popular approaches**:

* **SIMEX**: SIMulation EXtrapolation, a heuristic and intuitive idea.
* **Bayesian methods**: Prior information about the error enters a model. Then use

$$\text{Likelihood} \times \text{prior} = \text{posterior}$$

to calculate the parameter distribution after error correction.



\colorbox{lightgray}{\begin{minipage}{13.5cm}
{In any case, assessing the biasing effect of the error, as well as error modeling, can be done \alert{only if the error structure (model) and the respective model parameters} (e.g., error variances) \alert{are known!}} 
\end{minipage}}


Therefore: Information about the error mechanism is essential, and potential errors must be identified already in the planning phase.



## SIMEX: A very intuitive idea

Suggested by Cook & Stefanski (1994).

Idea:

* **Simulation phase:** The error in the data is \alert{progressively aggravated} in order to determine how the quantity of interest is affected by the error.
* **Extrapolation phase:** The observed trend is then \alert{extrapolated back} to a hypothetical error-free value.




## Illustration of the SIMEX idea

Parameter of interest: $\beta_x$ (e.g. a regression slope).

Problem: The respective covariate $x$ was estimated with error: $$w=x+u\ , \quad u\sim N(0,\sigma_u^2)$$

```{r, message=FALSE, fig.width=3, fig.height=2, fig.align='center'}
set.seed(212356)
sigmax <- 1
sigmau <- 0.25

# number of measurements
n <- 4

xx <- seq(1,3,0.25)/4#c(1,4/3,4/2,4/1) / 4
xx0 <- seq(0,3,0.05)
yy <- sigmax/(sigmax + xx)+rnorm(length(xx),0,0.03)
yy0 <- sigmax/(sigmax + xx0)

par(mar=c(5,4,0.1,0.1), cex=0.5)
plot(4*xx,yy,ylim=c(0.4,1.1),xlim=c(0,3.25),xlab=expression(sigma[u]^2),ylab=expression(beta[x]),cex.lab=1.24,cex=1.2)
#axis(1,tick=T,at=c(0,xx*4))#,labels=c(0,1,1.33,2,4))
points(4*xx[1],yy[1],pch=18,cex=2.5,col=2)
points(0,1,pch=18,cex=2.5,col=3)
legend("topright",legend=c("Naive","Corrected"),pch=18,col=c(2,3),cex=1.2)
lines(4*xx0,yy0,lty=2,lwd=2)
abline(v=1,lty=3,lwd=1)


```



## Example of SIMEX use (part 1)

Let's consider a linear regression model

$y_i = \beta_0 + \beta_x x_i + \beta_z z_i + \epsilon_i$ , $\epsilon_i = N(0,\sigma^2)$

with

* $\mathbf{y}=(y_1,\ldots, y_{100})^\top$: variable with \% Bodyfat of 100 individuals.
* $\mathbf{x}=(x_1,\ldots, x_{100})^\top$ the BMI of the individuals.

**Problem:* The BMI was self-reported and thus suffers from measurement error! Not $x_i$ are observed, but rather 
$$w_i = x_i + u_i \ , \quad u_i \sim N(0,4)$$ 

* $\mathbf{z}=(z_1,\ldots, z_{100})^\top$ a binary covariate that indicates if the $i$-th person was a male ($z_i=1$) or female ($z_i=0$).


$\rightarrow$ Apply the SIMEX procedure!



## Example of SIMEX use (part 2)

```{r}

set.seed(3243445)
x <- rnorm(100,24,4)
w <- x + rnorm(100,0,2)
z <- ifelse(x>25,rbinom(100,1,0.7),rbinom(100,1,0.3))
y <- -15 + 1.6*x - 2*z + rnorm(100,0,3)
data <- data.frame(cbind(w,z,y))
names(data) <- c("BMI","sex","bodyfat")
# summary(lm(y ~ x + z))
# summary(lm(y ~ w + z))

```



Use the error-prone BMI variable to fit a "naive" regression:

```{r}
r.lm <- lm(bodyfat ~ BMI + sex,data,x=TRUE)
summary(r.lm)$coef
```




Then run the simex procedure using the `simex()` function from the respective package:

```{r}
library(simex)
r.simex <- simex(r.lm,
                 SIMEXvariable = "BMI",
                 measurement.error = sqrt(4),
                 lambda = seq(0.1,2.5,0.1),
                 B = 100,
                 fitting.method = "quadratic")
summary(r.simex)$coef$asymptotic

```



## Graphical results with quadratic extrapolation function:


```{r echo = FALSE, message=FALSE, fig.width=5, fig.height=3, fig.align='center'}
par(mfrow = c(1,3), mar = c(4,4,2,1))
plot(r.simex)

```



**Note:** The `sex` variable has \emph{not} been mismeasured, nevertheless it is affected by the error in BMI!
**Reason:** `sex` and BMI are correlated.




## Practical advice

* Think about error problems **before** you start collecting your data!
* Ideally, take **repeated measurements**, maybe of a subset of data points.
* Figure out if error is a problem and what the bias in your parameters might be. You might need simulations to find out.
* If needed, model the error. **Seek help from a statistician!**





## References



Carroll, R. J., D. Ruppert, L. A. Stefanski, and C. M. Crainiceanu (2006). Measurement Error in Nonlinear Models: A Modern Perspective (2 ed.). Boca Raton: Chapman & Hall.

Cook, J. R. and L. A. Stefanski (1994). Simulation-extrapolation estimation in parametric measurement error models. Journal of the American Statistical Association 89, 1314–1328.



