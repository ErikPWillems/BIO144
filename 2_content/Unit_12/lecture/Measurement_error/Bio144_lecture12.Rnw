\documentclass[english,9pt,aspectraio=169]{beamer}
\usepackage{etex}
\usetheme{uzhneu-en-informal}
%\usepackage{uarial}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\RequirePackage{graphicx,ae}
\usepackage{bm}
\usepackage{fancybox,amssymb,color}
\usepackage{pgfpages}
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage{animate}
\usepackage{numprint}
\usepackage{vwcol} 
\usepackage{dsfont}
\usepackage{tikz}
\usepackage{amsmath,natbib}
\usepackage{mathbbol}
\usepackage{babel}
\usepackage{multicol}
\usepackage{xcolor}


\usetheme{uzhneu-en-informal}
\DeclareMathOperator{\po}{Poisson}
\DeclareMathOperator{\G}{Gamma}
\DeclareMathOperator{\Be}{Beta}
\DeclareMathOperator{\logit}{logit}
\def\n{\mathop{\mathcal N}}

\definecolor{Gray}{RGB}{139,137,137}
\definecolor{darkred}{rgb}{0.8,0,0}
\definecolor{Green}{rgb}{0,0.8,0.3}
\definecolor{lightgreen}{rgb}{0,0.7,0.3}
\definecolor{Blue}{rgb}{0,0,1}
\def\myalert{\textcolor{darkred}}
\def\myref{\textcolor{Gray}}
\setbeamercovered{invisible}

\renewcommand{\baselinestretch}{1.2}
\beamertemplateballitem
\DeclareMathOperator{\cn}{cn} % Copy number
\DeclareMathOperator{\ccn}{ccn} % common copy number
\DeclareMathOperator{\p}{p} % common copy number
\DeclareMathOperator{\E}{E} % common copy number
\DeclareMathOperator{\given}{|} % common copy number
\def\given{\,|\,}
\def\na{\tt{NA}}
\def\nin{\noindent}
\pdfpageattr{/Group <</S /Transparency /I true /CS /DeviceRGB>>}
\def\eps{\varepsilon}

\renewcommand{\P}{\operatorname{\mathsf{Pr}}} % Wahrscheinlichkeitsma√ü
\def\eps{\varepsilon}
\def\logit{\text{logit}}
%\newcommand{\E}{\mathsf{E}} % Erwartungswert
\newcommand{\Var}{\text{Var}} % Varianz
\newcommand{\NBin}{\text{NBin}}
\newcommand{\Po}{\text{Po}}
\newcommand{\N}{\mathsf{N}}
\newcommand{\odds}{\text{odds}}

\newcommand{\hl}{\textcolor{red}}

\newcommand{\ball}[1]{\begin{pgfpicture}{-1ex}{-0.65ex}{1ex}{1ex}
\usebeamercolor[fg]{item projected}

{\pgftransformscale{1.75}\pgftext{\normalsize\pgfuseshading{bigsphere}}}
{\pgftransformshift{\pgfpoint{0pt}{0.5pt}}
\pgftext{\usebeamerfont*{item projected}{#1}}}
\end{pgfpicture}}%
\usepackage{multicol}
\newcommand{\ballsmall}[1]{\begin{pgfpicture}{-1ex}{-0.65ex}{.2ex}{.2ex}

{\pgftransformscale{1}\pgftext{\normalsize\pgfuseshading{bigsphere}}}
{\pgftransformshift{\pgfpoint{0pt}{0.5pt}}
\pgftext{\usebeamerfont*{item projected}{#1}}}
\end{pgfpicture}}%



\begin{document}
\SweaveOpts{concordance=TRUE}
\fboxsep5pt
<<setup, include=FALSE, cache=FALSE, results='hide'>>=
library(knitr)
## set global chunk options
opts_chunk$set(fig.path='figure/', 
cache.path='cache/', fig.align='center', 
fig.show='hold', par=TRUE, fig.align='center', cache=FALSE, 
message=FALSE, 
warning=FALSE,
echo=FALSE, out.width="0.4\\linewidth", fig.width=6, fig.height=4.5, size="scriptsize", width=40)
opts_chunk$set(purl = TRUE)
knit_hooks$set(purl = hook_purl)
options(size="scriptsize")
opts_chunk$set(message = FALSE)
@

\frame{
\title[]{ \centering \Huge Kurs Bio144: \\
Datenanalyse in der Biologie}%\\[.3cm]
\author[Stefanie Muff, Owen L.\ Petchey]{\centering Stefanie Muff  \& Owen L.\ Petchey }
%\institute[]{Institute of Social and Preventive Medicine \\ Institute of Evolutionary Biology and Environmental Studies}
\date[]{Lecture 12 (part 2): Measurement error in regression models \\ 20. May 2019}


\maketitle
}


\frame{\frametitle{Overview}
\begin{itemize}
\item ME in covariates ($x$) and in the response ($y$) of regression models.
\item Effects of ME on regression parameters.
\item  When do I have to worry?
\item Simple methods to correct for ME. 
\end{itemize}
<<echo=F>>=
library(dplyr)
library(ggplot2)
library(ggfortify)
@
}


\frame{\frametitle{Course material covered today}

The lecture material of today is partially based on the following literature:\\[4mm]

\begin{itemize}
\item Chapter 6.1 in ``Lineare regression'' (BC reading)
\end{itemize}
}


\frame{
	\frametitle{Sources of measurement uncertainty / measurement error (ME)}

	\begin{itemize}
		\item {\bf Measurement imprecision} in the field or in the lab (length, weight, blood pressure, etc.).\\[0.2cm]
		\item Errors due to {\bf incomplete} or {\bf biased observations} (e.g., self-reported dietary aspects, health history).\\[0.2cm]
		\item Biased observations due to {\bf preferential sampling or repeated observations}.\\[0.2cm]
		\item Rounding error, digit preference.\\[0.2cm]
		\item {\bf Misclassification error} (e.g., exposure or disease classification).\\[0.2cm]
		\item \ldots \\[0.6cm]
	\end{itemize}
	\begin{center}
	``Error'' is often used synonymous to ``uncertainty''.
	\end{center}
}


\frame{\frametitle{The fundamental assumptions of regression analyses}

A fundamental assumption always is that some {\bf distributional assumptions are fulfilled}, for example:\\[4mm]

\begin{itemize}
\item {\bf Linear regression (including ANOVA):} 
$$\epsilon_i \sim \N(0,\sigma^2) \ . $$
\item {\bf Generalized linear model:} Implicit assumptions that can be checked by model diagnostic plots.\\[4mm]
\end{itemize}


}


\frame{\frametitle{Another fundamental assumption (often neglected!)}

\begin{itemize}
\item It is a \alert{fundamental assumption}  that explanatory variables are measured or estimated \alert{without error}, for instance for 
\begin{itemize}
\item the calculation of correlations.
\item linear regression and ANOVA.
\item Generalized linear and non-linear regressions (e.g.\ logistic and Poisson).\\[2mm]
\end{itemize}
\item Violation of this assumption may lead to \alert{biased} parameter estimates, altered standard errors and $p$-values, incorrect covariate importances, and to \alert{misleading conclusions}.\\[4mm]
\item Even standard statistics textbooks do often not mention these problems.\\[4mm]
\end{itemize}

$\rightarrow$ Measurement error in the covariates ($\bm{x}$) violates an assumption of standard regression analyses!!  

}


\frame{
	\frametitle{Classical measurement error}
	A very common error type:\\[2mm]
	
	Let \alert{$x_i$} be the {\bf correct but unobserved} variable and $w_i$ the observed proxy with error $u_i$. Then
	
	\vspace{-.3cm}
	\begin{columns}%[B]
	\begin{column}{.4\textwidth}
	\begin{eqnarray*}
	w_i &=& x_i + u_i \nonumber\\
	u_i &\sim& \N(0,\sigma^2_{u})  \  \label{CErrorModel} \ , \nonumber
	\end{eqnarray*}
	is the {\bf classical ME model}.
	\end{column}
	\begin{column}{.4\textwidth}
	\vspace*{-3cm}
	\begin{center}
	\hspace*{-3.5cm}
<<classicalError,fig.width=3, fig.height=3,out.width="6cm">>=
tx<-seq(-4,4,0.01)
par(mfrow=c(1,1))
plot(tx,dnorm(tx,0,1),type="l",xaxt="n",yaxt="n",xlab="",ylab="")
abline(v=0,lty=2,lwd=0.5)
text(0,0.02,labels=expression(x[i]),cex=0.6)
text(-1.5,0.3,labels=expression(w[i]),cex=0.6)
@
	\end{center}
	\vspace*{-3.5cm}
	\end{column}
	\end{columns}
	
	
\vspace{1cm}
	\structure{Examples:}   Imprecise measurements of a concentration, a mass, a length etc. \\[2mm]
$\rightarrow$The observed value $w_i$ varies around the true value $x_i$. \\
	
}





\frame{\frametitle{Illustration of the problem}
	~\\
	Find regression parameters $\beta_0$ and $\beta_x$ for the model with covariate $\bm{x}$:
	\begin{eqnarray*}
	y_i= 1 \cdot x_i + \epsilon_i, \quad \epsilon_i \sim \N(0,\sigma^2) \ .
	\end{eqnarray*}


	\vspace{1mm}

<<plot,fig.width=4.5, fig.height=3.5,out.width="6cm">>=
library(ggplot2)
set.seed(84522)
col1 <- "red"
col2 <- "blue"
n <- 100
beta_0 <- 0
beta_1 <- 1
epsilon <- rnorm(n, 0, 0.2)
x <- rnorm(n, 0, 1)
u <- rnorm(n, 0, 1)
w <- x + u
##Classical
y <- beta_0 + beta_1*x + epsilon
m1 <- lm(y~x)
m2 <- lm(y~w)
#nf <- layout(matrix(c(1,2),1,2,byrow=TRUE), c(4,4), c(4,2), TRUE)
#par( mar=c(4,6,4,1), cex.lab=1.5, cex.axis=1.4, cex.main=1.8)
cols <- c("X"="black","W"="red")
ggplot(mapping=aes(x=x,y=y,col="X")) +  geom_smooth(method="lm") + geom_point(size=0.9) + 
  # geom_smooth(mapping=aes(x=w,y=y,color="W"),method="lm") +  geom_point(mapping=aes(x=w,y=y,color="W"),size=0.9) + 
   scale_colour_manual(name="Variable",values=cols) +
   xlab("X") +
  xlim(c(-3.5,3.5)) +
  ylim(c(-2.7,2.7)) +
  theme_bw()
@

}



\frame{\frametitle{Illustration of the problem II}
	~\\

		However, assume that only an erroneous proxy $\bm{w}$ is observed with classical ME
	\begin{eqnarray*}
	w_i = x_i + u_i \ , \qquad u_i \sim \N(0,\sigma_u^2) \quad \text{with } \quad \sigma^2_u = \sigma^2_x \ .
	\end{eqnarray*}

	\vspace{1mm}

<<plot2,fig.width=4.5, fig.height=3.5,out.width="6cm">>=
library(ggplot2)
set.seed(84522)
col1 <- "red"
col2 <- "blue"
n <- 100
beta_0 <- 0
beta_1 <- 1
epsilon <- rnorm(n, 0, 0.2)
x <- rnorm(n, 0, 1)
u <- rnorm(n, 0, 1)
w <- x + u
##Classical
y <- beta_0 + beta_1*x + epsilon
m1 <- lm(y~x)
m2 <- lm(y~w)
#nf <- layout(matrix(c(1,2),1,2,byrow=TRUE), c(4,4), c(4,2), TRUE)
#par( mar=c(4,6,4,1), cex.lab=1.5, cex.axis=1.4, cex.main=1.8)
cols <- c("X"="black","W"="red")
ggplot(mapping=aes(x=x,y=y,color="X")) +  geom_smooth(method="lm") + geom_point(size=0.9) + 
  geom_smooth(mapping=aes(x=w,y=y,color="W"),method="lm") +  geom_point(mapping=aes(x=w,y=y,color="W"),size=0.9) + 
  scale_colour_manual(name="Variable",values=cols) +
  xlab("X or W") +
  theme_bw()
@

}


\frame{\frametitle{Simulations and apps}

% {\bf Bias the regression parameters}, mainly attenuation (underestimation) of the true effect.\\[2mm]

Illustration with shiny apps for two error types in linear, logistic and Poisson regression:\\[6mm]

\href{https://stefaniemuff.shinyapps.io/MEC_ChooseL/}
{\beamergotobutton{Classical error}}

\href{https://stefaniemuff.shinyapps.io/MEB_ChooseL/}
{\beamergotobutton{Berkson error}}

}

 

\frame{\frametitle{ The ``Triple Whammy of Measurement Error''}
{\small \citep{carroll.etal2006}}\\[2mm]

\begin{enumerate}
\item \alert{Bias}: The inclusion of erroneous variables in downstream analyses may lead to biased parameter estimates. \\[4mm]

\item ME leads to a \alert{loss of power} for detecting signals.\\[4mm]

\item ME \alert{masks imporant features} of the data, making graphical model inspection difficult.\\
\end{enumerate}

 
<<plot3,fig.width=4, fig.height=3,out.width="5.5cm">>=
library(ggplot2)
set.seed(84522)
col1 <- "red"
col2 <- "blue"
n <- 100
beta_0 <- 0
beta_1 <- 1
epsilon <- rnorm(n, 0, 0.2)
x <- rnorm(n, 0, 1)
u <- rnorm(n, 0, 1)
w <- x + u
##Classical
y <- beta_0 + beta_1*x + epsilon
m1 <- lm(y~x)
m2 <- lm(y~w)
#nf <- layout(matrix(c(1,2),1,2,byrow=TRUE), c(4,4), c(4,2), TRUE)
#par( mar=c(4,6,4,1), cex.lab=1.5, cex.axis=1.4, cex.main=1.8)
cols <- c("X"="black","W"="red")
ggplot(mapping=aes(x=x,y=y,color="X")) +  geom_smooth(method="lm") + geom_point(size=0.9) + 
  #geom_smooth(mapping=aes(x=w,y=y,color="W"),method="lm") +  
  geom_point(mapping=aes(x=w,y=y,color="W"),size=0.9) + 
  scale_colour_manual(name="Variable",values=cols) +
  xlab("X or W") +
  theme_bw()
@

}






% \frame{\frametitle{Error in the outcome of regression models }
% Example: \alert{Continuous} error in a linear regression outcome.\\[2mm]
% Observe 
% \begin{equation*}
% s_i = y_i + v_i \quad v_i \sim \N(0,\sigma_v^2)  
% \end{equation*}
% instead of $y_i$. 
% 
% $\rightarrow$ the error variance is simply absorbed in the residual variance. \\[-3mm]
% 
%  
% <<plot4, fig.width=7, fig.height=4.0,out.width="8cm">>=
% par(mfrow=c(1,2))
% x <- runif(100,-2,2)
% y <- x + rnorm(100,0,1)
% ystar <- y + rnorm(100,0,2)
% plot(y~x,xlab="",ylab="",main="",ylim=c(-6,6))
% mtext("without response error",3,cex=1.2,padj=-1)
% abline(c(0,1))
% plot(ystar~x,xlab="",ylab="",main="",ylim=c(-6,6))
% mtext("with response error",3,cex=1.2,padj=-1)
% abline(c(0,1))
% @
% \vspace{-5mm}
% $\rightarrow$ Error in the response seems to be less of a problem. However, this is \emph{not} true for other regression types (logistic, Poisson) or other error structures.
% }

\frame{\frametitle{How to correct for error?}
\begin{itemize}
\item Generally, to correct for the error we need an {\bf error model} and knowledge of the {\bf error model parameters}.\\[2mm]
{\bf Example}: If classical error $w_i = x_i + u_i$ with $u_i \sim \N(0,\sigma_u^2)$ is present, knowledge of the {\bf error variance} $\sigma_u^2$ is needed.\\[2mm]

{\bf Strategy}: Take repeated measurements to estimate the error variance!\\[4mm]

\item In \alert{simple cases}, formulas for the bias exist. \\[2mm]

\item In most cases, such simple relations don't exist. Specific error modeling methods are then needed!
\end{itemize}

}


\frame{\frametitle{Attenuation in simple linear regression}

Given the simple linear regression equation $y_i = \beta_0 + \beta_x x_i + \epsilon_i$ with $w_i = x_i + u_i$.
Assume that $w_i$ instead of $x_i$ is used in the regression:
$$y_i = \beta^\star_0 + \beta^\star_x w_i + \epsilon_i \ .$$
~\\

The {\bf naive slope parameter $\beta_x^\star$} is then underestimated with respect to the true slope $\beta_x$, with {\bf attenuation factor} $\lambda$: 
$$\beta_x^\star =\underbrace{ \left(\frac{\sigma_x^2}{\sigma_x^2 + \sigma_u^2}\right)}_{=\lambda} \beta_x \ .$$
$\rightarrow$ knowing $\sigma_u^2$ and $\sigma_x^2$, the correct slope can be retrieved!\\[2mm]

{\bf Example:} $\sigma_x^2 = 5$, $\sigma_u^2 = 1$ $\rightarrow$  $\lambda = \frac{5}{6} = 0.83 $.
}

% \frame{\frametitle{ME modelling: The SIMEX idea}
% One rather intuitive and general approach towards error modeling is the so-called {\bf SIMulation EXtrapolation (SIMEX)} approach.
%  
% }


\frame{\frametitle{Error modeling}

The {\bf two most popular approaches}:\\ 
\begin{itemize}
\item {\bf SIMEX}: SIMulation EXtrapolation, a heuristic and intuitive idea.\\[2mm]
\item {\bf Bayesian methods}: Prior information about the error enters a model. Then use $$\text{Likelihood} \times \text{prior} = \text{posterior}$$
to calculate the parameter distribution after error correction.\\[2mm]
\end{itemize}
 
 \vspace{3mm}
\colorbox{lightgray}{\begin{minipage}{10cm}
{ In any case, assessing the biasing effect of the error, as well as error modeling, can be done \alert{only if the error structure (model) and the respective model parameters} (e.g., error variances) \alert{are known!}} 
\end{minipage}}

\vspace{3mm}

Therefore: Information about the error mechanism is essential, and potential errors must be identified already in the planning phase.

}

\frame{\frametitle{SIMEX: A very intuitive idea}
Suggested by \citet{cook.stefanski1994}.\\[8mm]

Idea: \\[2mm]
\begin{itemize}
\item {\bf Simulation phase:} The error in the data is \alert{progressively aggravated} in order to determine how the quantity of interest is affected by the error.\\[4mm]
\item {\bf Extrapolation phase:} The observed trend is then \alert{extrapolated back} to a hypothetical error-free value.\\[2mm]
\end{itemize}
}


\frame[containsverbatim]{\frametitle{Illustration of the SIMEX idea}

Parameter of interest: $\beta_x$ (\emph{e.g.} a regression slope).\\
Problem: The respective covariate $x$ was estimated with error: $$w=x+u\ , \quad u\sim \N(0,\sigma_u^2) \ .$$
 
<<simex,fig.width=5, fig.height=4,out.width="6cm">>=
set.seed(212356)
sigmax <- 1
sigmau <- 0.25

# number of measurements
n <- 4

xx <- seq(1,3,0.25)/4#c(1,4/3,4/2,4/1) / 4
xx0 <- seq(0,3,0.05)
yy <- sigmax/(sigmax + xx)+rnorm(length(xx),0,0.03)
yy0 <- sigmax/(sigmax + xx0)

par(mar=c(4,5,2,1))
plot(4*xx,yy,ylim=c(0.4,1.1),xlim=c(0,3.25),xlab=expression(sigma[u]^2),ylab=expression(beta[x]),cex.lab=1.24,cex=1.2)
#axis(1,tick=T,at=c(0,xx*4))#,labels=c(0,1,1.33,2,4))
points(4*xx[1],yy[1],pch=18,cex=2.5,col=2)
points(0,1,pch=18,cex=2.5,col=3)
legend("topright",legend=c("Naive","Corrected"),pch=18,col=c(2,3),cex=1.2)
lines(4*xx0,yy0,lty=2,lwd=2)
abline(v=1,lty=3,lwd=1)
@
 
}

\frame[containsverbatim]{\frametitle{Example of SIMEX use}

Let's consider a linear regression model
\begin{equation*}
y_i = \beta_0 + \beta_x x_i + \beta_z z_i + \epsilon_i \ , \quad \epsilon_i = \N(0,\sigma^2)
\end{equation*}
with
\begin{itemize}
\item $\bm{y}=(y_1,\ldots, y_{100})^\top$: variable with \% Bodyfat of 100 individuals.\\[2mm]
\item $\bm{x}=(x_1,\ldots, x_{100})^\top$ the BMI of the individuals. \\
{\bf Problem:} The BMI was self-reported and thus suffers from measurement error! Not $x_i$ are observed, but rather 
$$w_i = x_i + u_i \ , \quad u_i \sim \N(0,4)  \ .$$ 
 
\item $\bm{z}=(z_1,\ldots, z_{100})^\top$ a binary covariate that indicates if the $i$-th person was a male ($z_i=1$) or female ($z_i=0$).\\[4mm]
\end{itemize}

$\rightarrow$ Apply the SIMEX procedure!
}

\frame[containsverbatim]{
<<echo=F,eval=T>>=
set.seed(3243445)
x <- rnorm(100,24,4)
w <- x + rnorm(100,0,2)
z <- ifelse(x>25,rbinom(100,1,0.7),rbinom(100,1,0.3))
y <- -15 + 1.6*x - 2*z + rnorm(100,0,3)
data <- data.frame(cbind(w,z,y))
names(data) <- c("BMI","sex","bodyfat")
# summary(lm(y ~ x + z))
# summary(lm(y ~ w + z))
@


Use the error-prone BMI variable to fit a ``naive'' regression:\\[2mm]
<<echo=T>>=
r.lm <- lm(bodyfat ~ BMI + sex,data,x=TRUE)
summary(r.lm)$coef
@


Then run the simex procedure using the \texttt{simex()} function from the respective package:\\[2mm]
<<echo=T>>=
library(simex)
r.simex <- simex(r.lm,SIMEXvariable="BMI",measurement.error=sqrt(4),
                 lambda=seq(0.1,2.5,0.1),B=100,fitting.method="quadratic")
summary(r.simex)$coef$asymptotic
@
}


\frame[containsverbatim]{
Graphical results with quadratic extrapolation function:
 
<<simexplot,fig.width=6, fig.height=3,out.width="10cm">>=
par(mfrow=c(1,3),mar=c(4,4,2,1))
plot(r.simex)
@


{\bf Note:} The \texttt{sex} variable has \emph{not} been mismeasured, nevertheless it is affected by the error in BMI!\\[2mm]
{\bf Reason:} \texttt{sex} and BMI are correlated.
}



\frame{\frametitle{Practical advice}
\begin{itemize}
\item Think about error problems {\bf before} you start collecting your data!\\[2mm]
\item Ideally, take {\bf repeated measurements}, maybe of a subset of data points.\\[2mm]
\item Figure out if error is a problem and what the bias in your parameters might be. You might need simulations to find out.\\[2mm]
\item If needed, model the error. {\bf Seek help from a statistician!}
\end{itemize}
}



\frame{References:
\bibliographystyle{Chicago}
\bibliography{refs}
}



\end{document}
