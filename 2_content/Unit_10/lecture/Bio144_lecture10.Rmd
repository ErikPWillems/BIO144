---
title: "Lecture 10: Modeling count data"
subtitle: "BIO144 Data Analysis in Biology"
author: "Stephanie Muff, Owen Petchey, Hanja Brandl"
institute: "University of Zurich"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  beamer_presentation:
    includes:
      in_header: ../../beamer_stuff/preamble.tex
classoption: "aspectratio=169"
---
  
```{r setup, include=FALSE, echo = FALSE, message=FALSE, warning=FALSE}
source(here::here("2_content/beamer_stuff/preamble.r"))
```

## Overview

```{r echo = FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(ggfortify)
```

\begin{itemize}
\item When the outcome ($y$) is a count\\[2mm]
\item Generalized linear models\\[2mm]
\item Poisson regression\\[2mm]
\item Link function\\[2mm]
\item Residual analysis / model checking / deviances\\[2mm]
\item Interpretation of the results\\[2mm]
\item Overdispersion, zero-inflation
\end{itemize}

## Course material covered today

The lecture material of today is based on the following literature:

\begin{itemize}
\item Chapter 7 of GSWR (Beckerman et al.)\\[2mm]
\end{itemize}

## Introduction

\begin{itemize}
\item We have seen: Explanatory variables in regression models can be \alert{continuous}, \alert{categorical} (binary or multi-level) or \alert{counted numbers} (integers).\\[2mm]
\item However, the \alert{response variable} (${y}$) was so far \alert{always continuous}, and the assumption was that the residuals $\epsilon_i\sim N(0,\sigma^2)$.\\[2mm]
\item \alert{In reality}, the response variable can also be a count (an integer $\geq 0$) or a categorical variable (e.g., presence/absence data).\\[2mm]
\item Today we will look at the case where the response variable is a \alert{count}, that is, $y_i =0,1,2,\dots$.\\[4mm]
\end{itemize}

## Count data

In biological or medical data, the outcome of interest is quite often a count:


\begin{itemize}
\item Counting items in time or space (animals, plants, species)\\[2mm]
\item Parasites in animals or humans\\[2mm]
\item Number of offspring in animals or humans\\[2mm]
\item Number of adverse health events (e.g., exacerbations) or health-related numbers (e.g., polyps)\\[4mm]
\end{itemize}

The research question then is: 

\textbf{How do the explanatory variables influence the probability of a certain count outcome?}

## Illustrative/working example: Soay sheep

Hirta, a small island of Scotland, is inhabited by an unmanaged and feral population of Soay sheep. 

Ecologists were interested in the \textbf{question} whether body mass of female animals influences their fitness, measured as their \alert{lifetime reproductive success} (the number of offspring over their lifespan).

\colorbox{lightgray}{\begin{minipage}{13cm}
\begin{center}
{\bf Question:} "Are heavier females fitter than lighter females?" 
\end{center}
\end{minipage}}

```{r echo = FALSE}
soay <- read.table(here("3_datasets/SoaySheepFitness.csv"),header = T, sep = ',')
```
\  

\small
```{r echo = TRUE}
glimpse(soay)
```

## 

As always, start with a graph (see GSWR book for code to produce the figure):

```{r echo = FALSE, fig.width=4, fig.height=4,out.width="6cm",fig.align='center',message=FALSE, warning=FALSE}
ggplot(soay, aes(x = body.size, y = fitness)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
geom_smooth(span = 1, colour = "red", se = FALSE) +
xlab("Body mass (kg)") + ylab("Lifetime fitness")
```

## 

\textbf{What can we see from the plot?}
\begin{itemize}
\item Reproductive success seems to increase with female body weight (not surprising, right?).\\[2mm]
\item The linear line seems unreasonable, the red (smoothed line) seems better. Maybe a quadratic term would be useful?\\[2mm]
\item However, the problem with these data is more subtle.\\[8mm]
\end{itemize}

\textbf{How does one analyze these data correctly?}
\begin{itemize}
\item So far, we always used linear regression.\\[2mm]
\item This is \alert{not} the correct approach here.\\[2mm]
\item We nevertheless start doing the `wrong' analysis!
\end{itemize}

## The "wrong" analysis

Use the `lm()` function to fit the linear model $y=\beta_0 + \beta_1 \cdot \text{bodySize} + \epsilon$ and look at the diagnostic plots:

```{r echo = FALSE, fig.width=6, fig.height=6,out.width="5.6cm",fig.align='center',message=FALSE,warning=FALSE}
r.soay <- lm(fitness ~ body.size ,data=soay)
autoplot(r.soay,smooth.colour = NA)
```

$\rightarrow$ The diagnostic plots indicate that the \textbf{linear regression assumptions are violated}!

## 

The same plot with smoothed lines (makes the problems more obvious):

```{r echo = FALSE, fig.width=6, fig.height=6,out.width="6.5cm",fig.align='center',message=FALSE,warning=FALSE}
autoplot(r.soay)
```

## 

What about the model with a quadratic term? 

\begin{equation*}
y=\beta_0 + \beta_1 \cdot \text{bodySize} + \beta_2 \cdot \text{bodySize}^2 + \epsilon
\end{equation*}

```{r echo = FALSE, fig.width=6, fig.height=6,out.width="6.5cm",fig.align='center',message=FALSE,warning=FALSE}
r2.soay <- lm(fitness ~ body.size + I(body.size^2),data=soay)
autoplot(r2.soay)
```

$\rightarrow$ This looks a bit better. However...

## What is the problem?

There is still a clear upward trend in the scale-location plot, indicating that the \alert{variance increases} as the fitted values grow larger (I used the smoother to make this obvious).


Moreover, the \alert{normal distribution} for count data is obviously \alert{violated}. Why?

\begin{itemize}
\item The normal distribution is for continuous variables.\\[2mm]
\item The normal distribution allows values $<0$, count data doesn't.\\[2mm]
\item The normal distribution is symmetrical, counts are often not! In particular, they cannot be negative.\\[10mm]
\end{itemize}

## A model for count data? I

Do you remember a distribution for count values from Mat183?

\colorbox{lightgray}{\begin{minipage}{14cm}
The probability distribution\footnote{A probability distribution is just a mathematical statement of how likely different events are, see GSWR book p.\ 169.} of Poisson-distributed random variable $Y$ with parameter $\lambda$ is defined as
\begin{equation*}
P(Y=k) = \frac{\lambda^k}{k!} e^{-\lambda} \ , \quad k=0, 1, 2,\ldots  \ .
\end{equation*}
In short: $$Y \sim Po(\lambda) \ .$$\\
\end{minipage}}

## A model for count data? II

\textbf{Characteristics of the Poisson distribution:}
\label{sl:poisson}
\begin{itemize}
\item Suitable to model unbounded counts ($k=0,1,2,...$).
\item $E(Y) = Var(Y) = \lambda$. In words: \\[2mm]
Mean = variance = $\lambda$.\\[2mm]

$\rightarrow$ The variance of the distribution increases with the mean.\\[4mm]
\end{itemize}

## A model for count data? III

Some examples:

\begin{center}
\includegraphics[width=10cm]{pictures/poisson.png}
\end{center}


So: How can one use the Poisson distribution for regression modeling?

## Doing it right: The generalized linear model (GLM) for count data

The aim of the GLM approach is that we can still use a \alert{linear predictor} $\eta_i$ in the form of the linear model:
\begin{equation*}
\eta_i = \beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \ldots + \beta_p x_i^{(p)} \ .
\end{equation*}


\begin{itemize}
\item In {\bf linear regression}, $\eta_i$ is the \alert{predicted value} for the mean $E(y_i)=\eta_i$.\\  
{\small (Why $E(y_i)$ and not $y_i$? Because the residual/error term $+ \epsilon_i$ is missing!)}\\[2mm]

\item However, for counts we cannot simply set $\eta_i$ equal to $E(y_i)$, if $y_i$ is a \alert{count}!\\[2mm]

\end{itemize}

## 

Why can't we set $E(y_i)=\eta_i$ for count data? 

\textbf{$\rightarrow$ Because nothing prevents $\eta_i$ from being negative!} 

Let us try to use the same approach as in linear regression, assuming that $E(y_i)=\eta_i$ for our soay sheep counts, that is,

$$E(y_i) = \beta_0 + \beta_1\cdot \text{bodySize}_i \ ,$$

using a model with $\beta_0=-2$ and $\beta_1=1.2$.

What is the predicted number of offspring for a 1kg sheep? Plug-in: $-2 + 1.2\cdot 1 = -0.8$, thus a negative prediction!

This is very unreasonable, right?

$\rightarrow$ We need a trick!

## The trick: Use a link function 

Instead of using $E(y_i)=\eta_i$ as in linear regression, we simply log-transform the expected value.

\colorbox{lightgray}{\begin{minipage}{14cm}
In order to prevent the expected value $E(y_i)$ to be negative for count data $y_i$, we use
\begin{equation}\label{eq:logLink}
\log(E(y_i)) = \eta_i  = \beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \ldots + \beta_p x_i^{(p)}\ .
\end{equation}
\end{minipage}}

\begin{itemize}
\item The $\log$ is called the \alert{link function}.\\[2mm]
\item The {\bf advantage}: The predicted fitness $E(y_i)$ is now \alert{always positive}, because equation \eqref{eq:logLink} is identical to 
$$E(y_i) = \exp(\beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \ldots + \beta_p x_i^{(p)})  \ ,$$
which is now {\bf always} $>0$! {\scriptsize (Plot the $\exp()$ function if you forgot what it looks like...)}
\end{itemize}

## The probability model 

\begin{itemize}
\item Finally, we need a reasonable \alert{probability model for the response variable}. \\[3mm
]
\item Remember: we always used the normality assumption $y_i \sim N(\eta_i, \sigma^2)$ in linear regression.\\[3mm]

\item Given that $y_i$ are counts, a Poisson model seems more reasonable:  
$$y_i \sim Po (E(y_i)) \ .$$

In words: $y_i$ is a realization of a Poisson random variable distributed as $Po(\lambda_i)$, where $\lambda_i=E(y_i)$.\\[3mm]
\item We say: The model belongs to the Poisson \alert{family}.
\end{itemize}

## Key terms for GLMs

In summary, we have introduced three terms related to GLMs:

\begin{itemize}
\item \emph{\bf Family}: The family corresponds to the likelihood model that is used for the (transformed) response. Here, the family is \alert{Poisson}. Another very common distribution is the \alert{binomial} family for binary outcome (see next week). Other families are the gamma or the negative binomial. The family is determined by the data type!\\[2mm]

\item \emph{\bf Linear predictor}: The linear predictor is always given as
\begin{equation*}
\eta_i = \beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \ldots + \beta_p x_i^{(p)} \ .
\end{equation*}
In the soay sheep example, it is $\eta_i = \beta_0 + \beta_1\cdot \text{bodySize}_i$ for sheep $i$.\\[2mm]

\item \emph{\bf Link function}: It defines how the linear predictor $\eta_i$ is \alert{related} to $E(y_i)$. In Poisson regression this is typically the log: $\log(E(y_i)) = \eta_i$! We will see another important link function next week (for binary data).
\end{itemize}

## Doing it right: Fitting a Poisson GLM

Uff... that was hard! But we finally have all the tools for fitting a Poisson GLM. A statistician would now say: 
\begin{center}
"Let's fit a Poisson GLM with a log-link!"\\[4mm]
\end{center}

\begin{itemize}
\item Basically, the idea is again to perform \alert{maximum-likelihood estimation}, but the details are a bit more complicated (nothing for us).\\[4mm]

\item Luckily, there is an R-function that works (almost) like `lm()`, namely \alert{\texttt{glm()}} :

\texttt{soay.glm <- glm(fitness ~ body.size, data = soay, family = poisson(link=log))}

You {\bf must} specify the \alert{family}, but you could leave away the \texttt{link=log} option here, because R automatically picks the log link for the Poisson family...

\end{itemize}

\small
```{r echo = FALSE}
soay.glm <- glm(fitness ~ body.size, data = soay, family = poisson(link=log))
```

## Doing it right: Model diagnostics

Before we look at the output, let's do some model diagnostics (as we did it for linear regression):

```{r echo = FALSE, fig.width=5.5, fig.height=5.5,out.width="6.2cm",fig.align='center',message=FALSE,warning=FALSE}
autoplot(soay.glm,smooth.colour = NA)
```

## 

\begin{itemize}
\item Model diagnostics seem ok. In particular, the scale location plot is a bit better than when using the quadratic term in linear regression (slide 11).\\[2mm]

\item Do you find it strange that the same diagnostic plot can be used as in simple linear regression?\\[2mm]

\item The definition of a "residuum" is no longer clear when link-functions are used (on which scale should residuals be calculated? On the link scale or on the observed scale?)\\[2mm]

\item We don't care too much about this aspect, but you should remember:\\[2mm]

\begin{itemize}
\item There are \alert{different types of residuals}.\\[2mm]
\item \texttt{autoplot()} \alert{automatically picks} the residuals that ``make sense''.
\end{itemize}
\end{itemize}

## Doing it right: Interpreting the coefficients

Let's now look at the `summary()` output (yes! this works also for glm objects):

\tiny
```{r echo = TRUE, }
summary(soay.glm)
```

## 

You see several (familiar and less familiar) components in the output. For the moment, we are interested in the coefficients, which are estimated as 


\begin{equation*}
\hat\beta_0 = `r format(summary(soay.glm)$coef[1,1],3,3,3)` \quad \text{and}\quad \hat\beta_1 = `r format(summary(soay.glm)$coef[2,1],3,3,3)`
\end{equation*}


with respective standard errors and $p$-values. In particular, $p<0.001$ for $\hat\beta_1$ indicates \alert{very strong evidence for a positive (because $\hat\beta_1 >0$!) effect  of female weight on reproductive success} (number of offspring)!

\textbf{Good to know:} Theory says that the estimated coefficients $\hat\beta_0$ and $\hat\beta_1$ are approximately \alert{normally distributed} around the true values:

\begin{equation}
\hat\beta \sim N(\beta,\sigma_\beta^2)
\end{equation}

Thus a 95\% CI can be approximated by the usual $\hat\beta\pm 2\cdot \hat\sigma_\beta$ idea.

## What do the coefficients tell us?

Remember our model
$$
\log(E(y_i)) =\beta_0 + \beta_1 \cdot \text{bodySize}_i
$$
Thus the $\exp(\cdot)$ transformation leads to 
$$ \quad E(y_i) = \exp(\beta_0 + \beta_1 \cdot \text{bodySize}_i) \ .$$
Our estimates $\hat\beta_0$ and $\hat\beta_1$ can be plugged into this equation!

For example, a 5~kg female has \textbf{expected fitness} (i.e., produces)
$$\exp(`r format(summary(soay.glm)$coef[1,1],3,3,3)` + 
`r format(summary(soay.glm)$coef[2,1],3,3,3)`\cdot 5) = 
`r format(exp(summary(soay.glm)$coef[1,1] + summary(soay.glm)$coef[2,1]*5),2,2,2)` 
\text{ lambs} \ ,$$

while a 7~kg female would already have \textbf{expected fitness} 
$$\exp(`r format(summary(soay.glm)$coef[1,1],3,3,3)` + `r  format(summary(soay.glm)$coef[2,1],3,3,3)`\cdot 7) = `r format(exp(summary(soay.glm)$coef[1,1] + summary(soay.glm)$coef[2,1]*7),2,2,2)` \text{ lambs} \ .$$ 

Makes sense, right?

## Doing it right: The \texttt{anova()} table

Remember: We were sometimes using ANOVA tables, for instance for linear regression output with categorical explanatory variables.

\texttt{anova()} is useful again, but it gives us the \alert{Analysis of Deviance} table:

\small
```{r echo = TRUE}
anova(soay.glm)
```

\textbf{Note:} The deviance is essentially a difference of likelihoods.
In brief, the larger the likelihood, the better fits your model to the data...


## 

Here, the \alert{total deviance} is given by the so-called \alert{NULL} deviance: 85.081. It is the analogon to the total variability of the data in linear regression...

Of this, 37.041 is \alert{explained by bodysize}.

The question is, whether this is ``much''? This can be tested by a $\chi^2$ test, because the value is $\chi^2$ distributed with 1 degree of freedom:

```{r echo = TRUE}
pchisq(37.041,1,lower.tail=F)
```

You would get the same if you directly specify the test you want to carry out in the \texttt{anova()} call:

\small
```{r echo = TRUE, eval = FALSE}
anova(soay.glm, test="Chisq")
```

## Making a beautiful graph

See section 7.4.5 in the GSWR book. Work through the code to obtain this one:

```{r echo = FALSE, message=FALSE, warning=FALSE, fig.width=4, fig.height=4,out.width="6cm",fig.align='center',message=FALSE,warning=FALSE}
min.size <- min(soay$body.size)
max.size <- max(soay$body.size)
 
new.x <- expand.grid(body.size =seq(min.size, max.size, length=1000))

new.y = predict(soay.glm, newdata = new.x, se.fit = TRUE)
new.y = data.frame(new.y)
addThese <- data.frame(new.x, new.y)
addThese <- mutate(addThese, fitness = exp(fit),
lwr = exp(fit - 1.96 * se.fit),
upr = exp(fit + 1.96 * se.fit))
 
ggplot(soay, aes(x = body.size, y = fitness)) +
# first show the raw data
geom_point(size = 3, alpha = 0.5) + 
geom_smooth(data = addThese, aes(ymin = lwr, ymax = upr), stat = 'identity') +
theme_bw()
```

## Your turn!

Let's look at another data example, taken from Hothorn and  Everitt (2014), chapter 7:

A new drug was tested in a clinical trial (Giardiello et al. 1993, Piantodsi 1997), aiming at \alert{reducing the number of polyps} in the colon (Dickdarm). The data are publicly available from the Hothorn/Everitt book package:
\  
  
  
```{r echo = TRUE, message=FALSE, warning=FALSE}
library(HSAUR3)
data("polyps")
```

\textbf{Scientific question:} Does the drug influence (reduce) the number of polyps?

## 

Data: Number of polyps (outcome), the binary variable for the treatment, and the continuous explanatory variable age.

\tiny
```{r echo = FALSE, message=FALSE, warning=FALSE, results='asis'}
library(xtable)
pol1 <- polyps[1:10,]
pol2 <- polyps[11:20,]
ttab1 <- xtable(pol1,digits=0)
ttab2 <- xtable(pol2,digits=0)
print(ttab1, floating = TRUE, include.rownames=FALSE)
print(ttab2,  floating = TRUE, include.rownames=FALSE)
```

## 

\textbf{Your tasks (in teams, if you like):}

Look at the analysis done on the next three slides, and answer the following questions:

\begin{enumerate}
\item Are there any problems visible from the diagnostics plots?\\[2mm]
\item Does the treatment seem to be effective? If yes, can you \alert{quantify the effect}?\\[2mm]
\item Is age a relevant variable? If yes, what happens when patients grow older?\\[2mm]
\end{enumerate}

## 

\small 
```{r echo = FALSE}
polyps.glm <- glm(number ~ treat + age,data=polyps, family="poisson")
```

```{r echo = FALSE, message=FALSE, warning=FALSE, fig.width=6, fig.height=6,out.width="7cm", fig.align='center'}
autoplot(polyps.glm,smooth.colour = NA)
```

## 

\tiny 
```{r echo = TRUE}
anova(polyps.glm,test="Chisq")
```

## 

\tiny 
```{r echo = TRUE}
summary(polyps.glm)
```

## Overdispersion

\textbf{"Overdispersion"} means \textbf{"extra variability"}. Why could this be a problem?

\textbf{Remember:} The variance of the Poisson distribution increases with the mean, because \alert{\textbf{mean=variance}} (see slide 14). 

In Poisson regression it is assumed that, for each observation $i$,
$$y_i \sim Po (E(y_i)) \ .$$

However, the \alert{variance is often larger than the mean} in reality, because there are factors that influence the response ("cause variability in it") that cannot be captured by the explanatory variables!  


\small 
Why? Maybe you simply cannot observe the variable or it is too expensive to monitor, or...

## Detecting overdispersion

Look at the summary output from your GLM object, check the "Residual deviance" and compare it to the "degrees of freedom".

\textbf{Soay sheep data:} Res. deviance: 48.040, df=48 


\textbf{Polyps data:} Res. deviance: 178.54, df=17

The residual deviance should be approximately $\chi^2$ distributed with df degrees of freedom. This means that one should check whether

\begin{center}
\colorbox{lightgray}{\begin{minipage}{10cm}
\begin{center}
\texttt{Residual deviance} $\approx$ \texttt{df} .
\end{center}
\end{minipage}}
\end{center}

The sheep data seem fine, but the polyps data have  

\texttt{Residual deviance}$>>$ \texttt{df}  $\quad\rightarrow$ overdispersion!

\tiny 
If unclear, use the $\chi^2$ test with \texttt{df} degrees of freedom.

## What is the problem with overdispersion?

\colorbox{lightgray}{\begin{minipage}{14cm}
When there is unaccounted overdispersion, the $p$-values that are calculated are usually \alert{too small}! 
\end{minipage}}  
\  

\textbf{Possible solutions:}

\begin{itemize}
\item Use as your regression family \alert{\texttt{quasipoisson}} instead of \texttt{family = poisson}. \\[2mm]

This allows to estimate the variance parameter (denoted as \alert{dispersion parameter}) separately.\\[2mm]

\item Use a \alert{negative binomial regression} (there is the \texttt{glm.nb()} fuction from the \texttt{MASS} package in R to fit it, check it out by \texttt{?glm.nb()}).
\end{itemize}
 
## Reanalyzing the polyps data with quasipoisson

\tiny
```{r echo = TRUE}
polyps.glm2 <- glm(number ~ treat + age,data=polyps, family="quasipoisson")
summary(polyps.glm2)
```

## 

\begin{itemize}
\item The \alert{dispersion parameter} is now estiamted as 10.73, which is calculated as  
\begin{center}
\texttt{Residual deviance / df}.\\[6mm]
\end{center}

\item The $p$-values for the coefficients are now larger! In particular, there is only weak evidence ($p=0.063$) for an effect of age!\\[2mm]
$\rightarrow$ The \texttt{poisson} family gave too optimistic $p$-values!\\[2mm]

We say: The $p$-values were \alert{anti-conservative} or non-conservative.\\[6mm]
\end{itemize}

\small 
(Anti-conservative results are really \textbf{the worst} that can happen. Guess why?)

## Underdispersion?

\textbf{Can it happen} that the observations are \alert{less variable} than expected?

\textbf{Yes}: Especially when there are \alert{dependent observations}!

You can \textbf{detect it} by checking if \; \texttt{Residual deviance} $<$ \texttt{df}.

In that case, your $p$-values are usually too large, that is, the results are \alert{conservative} (just the opposite of the overdispersion problem).

The \texttt{quasipoisson} regression is a pragmatic solution in that case, too.

## Zero-inflation

A special type of overdispersion may be caused by an \alert{overrepresentation of zeros} in the obervations.

\textbf {Example}: Numbers of cigarettes smoked

Some people are never-smokers, so they will always produce a zero obervation, while smokers may smoke any number of cigarettes. 
\ 
  
  
Please read chapter 7.5.2 of GSWR for some ideas how to handle this case.

## A note on interpretation and model selection

Just as a reminder:
\ 
  
  
\bf
The same remarks and warnings from the last weeks for linear models regarding
\begin{itemize}
\item Caution with model selection  
\item Interpretation of $p$-values
\item Reproducibility aspects
\end{itemize}
also apply to GLMs!

## Summary

\begin{itemize}
\item Poisson regression is useful to model counted outcomes.\\[2mm]
\item Pretending that counted outcomes are continuous may lead to wrong results.\\[2mm]
\item The main ingredients of GLMs are\\[1mm]
\begin{itemize}
\item The family\\[1mm]
\item The linear predictor\\[1mm]
\item The link function.\\[2mm]
\end{itemize}
\item Model diagnostics are similar as in the linear case (\texttt{autoplot()}).\\[2mm]
\item Interpret the coefficients by back-transforming to the original scale.\\[2mm]
\item Analysis of deviance is the analogon to ANOVA (\texttt{anova()}).\\[2mm]
\item Over-(under-)dispersion, how to detect it and what do do.
\end{itemize}

## References

Hothorn, T. and B.S. Everitt (2014). _A Handbook of Statistical Analyses Using R_ (3 ed.). Boca Raton:Chapman & Hall/CRC Press.
