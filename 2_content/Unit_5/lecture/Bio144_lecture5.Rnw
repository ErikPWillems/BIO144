\documentclass[english,9pt,aspectraio=169]{beamer}
\usepackage{etex}
\usetheme{uzhneu-en-informal}
%\usepackage{uarial}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\RequirePackage{graphicx,ae}
\usepackage{bm}
\usepackage{fancybox,amssymb,color}
\usepackage{pgfpages}
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage{animate}
\usepackage{numprint}
\usepackage{dsfont}
\usepackage{tikz}
\usepackage{amsmath,natbib}
\usepackage{mathbbol}
\usepackage{babel}
%\usepackage{SweaveSlides}
\usepackage{multicol}
\usepackage{xcolor}

\usepackage{hyperref}
\hypersetup{colorlinks,urlcolor=blue}

\usetheme{uzhneu-en-informal}
\DeclareMathOperator{\po}{Poisson}
\DeclareMathOperator{\G}{Gamma}
\DeclareMathOperator{\Be}{Beta}
\DeclareMathOperator{\logit}{logit}
\def\n{\mathop{\mathcal N}}

\definecolor{Gray}{RGB}{139,137,137}
\definecolor{darkred}{rgb}{0.8,0,0}
\definecolor{Green}{rgb}{0,0.8,0.3}
\definecolor{Blue}{rgb}{0,0,1}
\def\myalert{\textcolor{darkred}}
\def\myref{\textcolor{Gray}}
\def\myblue{\textcolor{Blue}}
\setbeamercovered{invisible}

\renewcommand{\baselinestretch}{1.2}
\beamertemplateballitem
\DeclareMathOperator{\cn}{cn} % Copy number
\DeclareMathOperator{\ccn}{ccn} % common copy number
\DeclareMathOperator{\p}{p} % common copy number
\DeclareMathOperator{\E}{E} % common copy number
\DeclareMathOperator{\given}{|} % common copy number
\def\given{\,|\,}
\def\na{\tt{NA}}
\def\nin{\noindent}
\pdfpageattr{/Group <</S /Transparency /I true /CS /DeviceRGB>>}
\def\eps{\varepsilon}

\renewcommand{\P}{\operatorname{\mathsf{Pr}}} % Wahrscheinlichkeitsma√ü
\def\eps{\varepsilon}
\def\logit{\text{logit}}
%\newcommand{\E}{\mathsf{E}} % Erwartungswert
\newcommand{\Var}{\text{Var}} % Varianz
\newcommand{\NBin}{\text{NBin}}
\newcommand{\Po}{\text{Po}}
\newcommand{\N}{\mathsf{N}}

\newcommand{\hl}{\textcolor{red}}

\newcommand{\ball}[1]{\begin{pgfpicture}{-1ex}{-0.65ex}{1ex}{1ex}
\usebeamercolor[fg]{item projected}

{\pgftransformscale{1.75}\pgftext{\normalsize\pgfuseshading{bigsphere}}}
{\pgftransformshift{\pgfpoint{0pt}{0.5pt}}
\pgftext{\usebeamerfont*{item projected}{#1}}}
\end{pgfpicture}}%
\usepackage{multicol}
\newcommand{\ballsmall}[1]{\begin{pgfpicture}{-1ex}{-0.65ex}{.2ex}{.2ex}

{\pgftransformscale{1}\pgftext{\normalsize\pgfuseshading{bigsphere}}}
{\pgftransformshift{\pgfpoint{0pt}{0.5pt}}
\pgftext{\usebeamerfont*{item projected}{#1}}}
\end{pgfpicture}}%




\begin{document}
 
\fboxsep5pt
<<setup, include=FALSE, cache=FALSE, results='hide'>>=
library(knitr)
## set global chunk options
opts_chunk$set(fig.path='figure/', 
cache.path='cache/', fig.align='center', 
fig.show='hold', par=TRUE, fig.align='center', cache=FALSE, 
message=FALSE, 
warning=FALSE,
echo=FALSE, out.width="0.4\\linewidth", fig.width=6, fig.height=4.5, size="scriptsize", width=40)
opts_chunk$set(purl = TRUE)
knit_hooks$set(purl = hook_purl)
options(size="scriptsize")
opts_chunk$set(message = FALSE)
@


\frame{
\title[]{ \centering \Huge Kurs Bio144: \\
Datenanalyse in der Biologie} 
\author[Stefanie Muff, Owen L.\ Petchey]{\centering Stefanie Muff  \& Owen L.\ Petchey }
\date[]{Lecture 5: Multiple linear regression (finalize) / Residual analysis / Checking modeling assumptions\\ 18. March 2019}


\maketitle
}


\frame{\frametitle{Overview }
\begin{itemize}
\item Interactions between covariates \\[2mm]
\item Multiple vs.\ many single regressions\\[2mm]
\item Checking assumptions / Model validation \\[2mm]
\item What to do when things go wrong?\\[2mm]
\item Transformation of variables/the response\\[2mm]
\item Handling of outliers
\end{itemize}
}


\frame{\frametitle{Course material covered today}
The lecture material of today is based on the following literature:\\[4mm]

\begin{itemize}
\item Chapters 3.2u-x, 3.3, 4.1-4.5 in \emph{Lineare Regression} 
\end{itemize}
}

\frame[allowframebreaks]{\frametitle{Recap of last week}
\begin{itemize}
\item Multiple linear regression model $y_i= \beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \ldots + \beta_m x_i^{(m)} + \epsilon_i $.\\[3mm]
\item \alert{Binary} and \alert{factor} covariates: The idea is to introduce \alert{dummy variables} such that
\colorbox{lightgray}{\begin{minipage}{10cm}
\begin{equation*}
x_i^{(j)} = \left\{ 
\begin{array} {ll}
1, & \text{if the $i$th observation belongs to group $j$}.\\
0, & \text{otherwise.}
\end{array}\right.
\end{equation*}
\end{minipage}}
~\\[1mm]

\item Include $x^{(2)}$, ... ,$x^{(k)}$ in the regression, given that $x^{(1)}$ is used as \alert{reference category} ($\beta_1=0$).\\[3mm]

\item The factor covariates of last week were used to allow for \alert{group-specific intercepts} (see earthworm example).\\[3mm]

\begin{center}
<<fig1,fig.width=5, fig.height=4,out.width="6.5cm">>=
library(ggplot2)
d.wurm <- read.table ("../../data_examples/ancova/Projekt6Regenwuermer/RegenwuermerDaten_Haupt.txt",header=T)
d.wurm[d.wurm$Gattung=="N","GEWICHT"] <- d.wurm[d.wurm$Gattung=="N","GEWICHT"]*0.5
r.lm <- lm(log10(GEWICHT) ~  MAGENUMF + Gattung,d.wurm)
cc <- r.lm$coefficients
ggplot(d.wurm,aes(x=MAGENUMF,y=log10(GEWICHT),colour=Gattung)) + geom_point() + geom_line(aes(y=cc[1]+cc[2]*MAGENUMF),colour=2)  + geom_line(aes(y=cc[1]+cc[3]+cc[2]*MAGENUMF),colour=3)  + geom_line(aes(y=cc[1]+cc[4] + cc[2]*MAGENUMF),colour=4) + theme_bw()
@
\end{center}

\item The \alert{$F$-test} is used to test if $\beta_2=\beta_3=...=\beta_k=0$ at the same time for a factor covariate with $k$ levels. Use the \texttt{anova()} function in R to carry out this test.\\[3mm]
\item The $F$-test is a \alert{generalization of the $t$-test}, because the latter is used to test $\beta_j = 0$ for one single variable $x^{(j)}$.
\end{itemize}

~\\[4mm]
\myblue{\large Cooking rule:}\\

\begin{itemize}
\item Test for a single $\beta_j=0$ $\rightarrow$ $t$-test.
\item Test for several $\beta_2 = ... = \beta_{k}=0$ simultaneously $\rightarrow$ $F$-test.\\
$\rightarrow$ \texttt{anova()} 
\end{itemize}

Thus you will __always__ need the $F$-test `anova()` to obtain a $p$-value for a factor covariate with more than 2 levels!
}



\frame[containsverbatim]{\frametitle{Group-specific slopes: Interactions}





It may happen that \alert{groups do not only differ in their intercept ($\beta_0$), but also in their slopes ($\beta_x$)}.\\[2mm]
In the earthworm example, allowing for different intercepts and slopes:
\begin{center}
<<fig2, eval=T,fig.width=5.5, fig.height=4,warning=FALSE,out.width="6.5cm">>=
library(ggplot2)
library(dplyr)
ggplot(d.wurm,aes(x=MAGENUMF,y=log10(GEWICHT),colour=Gattung)) + geom_point() + geom_smooth(method="lm") + theme_bw()
@
\end{center}
{\bf Important:} This model will be fitted in this week's BC videos.
}




\frame[containsverbatim]{\frametitle{Binary variable with interaction}

For simplicity, let us look at a binary covariate ($x_i \in \{0,1\}$). \\



Remember the mercury (Hg) example from last week. We now extended the dataset and include mothers \emph{and} children ($\leq 11$ years).\\[2mm]

It is known that Hg concentrations may change over the lifetime of humans. So let us look at \texttt{log(Hg$_\text{urin}$)} depending on the age of the participants:\\

\begin{center}
<<fig3, eval=T,fig.width=5.5, fig.height=3.7,warning=FALSE,out.width="6.5cm">>=
path <- "../../data_examples/Hg/"
d.hg <- read.table(paste(path,"Hg_urin.csv",sep=""),header=T, sep=",")
d.hg <- d.hg[,c(1,2,5,6,7,8,10)]
names(d.hg) <- c("Hg_urin", "Hg_soil", "smoking","amalgam", "age", "fish","mother")
d.hg <- mutate(d.hg,mother=factor(mother))
ggplot(d.hg,aes(x=age,y=log(Hg_urin),colour=mother)) + geom_point() + geom_smooth(method="lm") + theme_bw()
# plot(log(Hg_urin) ~ age, d.hg,col=mother+1,cex.lab=1.5)
# legend("topright",legend=c("children","mothers"),col=1:2,pch=1)
# abline(lm(log(Hg_urin)~age,data=subset(d.hg,mother==0)))
# abline(lm(log(Hg_urin)~age,data=subset(d.hg,mother==1)),col=2)
@
\end{center}
}

\frame{

Observation: {\bf The regression lines are not parallel.}\\[2mm]

$\rightarrow$ Children and mothers seem to depend differently on age!\\[8mm]


What does this mean for the model?\\[2mm]
 
$\rightarrow$ Formulate a model that allows for \alert{different intercepts \emph{and} slopes}, depending on group membership (mother/child).\\[2mm]

$\rightarrow$ This can be achieved by introducing a so-called \alert{interaction term} into the regression equation.
}

\frame{

The smallest possible model is then given as\\
\colorbox{lightgray}{\begin{minipage}{10cm}
\begin{equation}\label{eq:HgInt}
y_i =  \beta_0 + \beta_1 \text{mother}_i + \beta_2 \text{age}_i + \beta_3\text{age}_i\cdot \text{mother}_i + \epsilon_i \ , 
\end{equation}
\end{minipage}}
~\\
where $y_i=\log(Hg_{\text{urin}})_i$, and \texttt{mother} is a binary ``dummy'' variable that indicates if the person is a mother (1) or a child (0).\\[8mm]

This results in essentially {\bf two} models with group specific intercept and slope:\\[2mm]
\colorbox{lightgray}{\begin{minipage}{10cm}
Mothers ($x_i=1$): $\hat{y}_i = \beta_0 +  \beta_1 + (\beta_2 +\beta_3)\text{age}_i  $  \\[2mm]
Children ($x_i=0$): $\hat{y}_i = \beta_0  + \beta_2 \text{age}_i $  
\end{minipage}}
 
}




\frame[containsverbatim]{\frametitle{}
Fitting model \eqref{eq:HgInt} in R is done as follows, where \texttt{age:mother} denotes the interaction term ($\text{age}_i\cdot \text{mother}_i$):\\[2mm]

<<echo=F>>=
# path <- "~/Teaching/Bio144/data_examples/Hg/"
# d.hg <- read.table(paste(path,"Hg_urin.csv",sep=""),header=T, sep=",")
# d.hg <- d.hg[,c(1,2,5,6,7,8,10)]
# names(d.hg) <- c("Hg_urin", "Hg_soil", "smoking","amalgam", "age", "fish","mother")
@
<<echo=T>>=
r.hg <- lm(log(Hg_urin)~  mother + age + age:mother,d.hg)
summary(r.hg)$coef
@
~\\

Interpretation: \\[2mm]

Mothers: $\hat{y}_i = \Sexpr{round(r.hg$coefficients[1],2)} + (\Sexpr{round(r.hg$coefficients[2],2)}) + (\Sexpr{round(r.hg$coefficients[3],2)} + \Sexpr{round(r.hg$coefficients[4],2)}) \cdot \text{age}_i$ \\[2mm]
Children: $\hat{y}_i = \Sexpr{round(r.hg$coefficients[1],2)}  + (\Sexpr{round(r.hg$coefficients[3],2)}) \cdot \text{age}$ \\[2mm]

\begin{itemize}
\item The Hg level drops in young children.
\item The Hg level increases in adults (mothers).\\
\end{itemize}
}

\frame{
On the previous slide we have actually fitted 2 models at the same time. \\[4mm]

\begin{itemize}
\item What is the advantage of this? 
\item Why is this usually better than fitting two separate models, one for children and one for mothers?
\end{itemize} 

~\\[2mm]
$\rightarrow$ Clicker exercise \href{http://www.klicker.uzh.ch/bkx}{http://www.klicker.uzh.ch/bkx}

}

\frame[containsverbatim]{\frametitle{}
Remember (from last week), however, that the Hg model also included smoking status, amalgam fillings and fish consumption as important predictors. It is very straightforward to just include these predictors in model \eqref{eq:HgInt}, which leads to the following model \\

<<echo=T>>=
r.hg <- lm(log(Hg_urin)~  mother * age + smoking + amalgam + fish,d.hg)
@

<<results="asis">>=
library(biostatUZH)
tableRegression(r.hg)
@

{\small (Note that \texttt{mother*age} in R encodes for \texttt{mother} + \texttt{age} + \texttt{mother:age}.)}
}

\frame[containsverbatim]{
Again, for completeness, some model checking (which one usually does before looking at the results): \\[8mm]


\begin{center}
<<modelChecksHg, eval=T,fig.width=7, fig.height=3.5,warning=FALSE,out.width="8.5cm">>=
library(ggfortify)
autoplot(r.hg,which=c(1,2),smooth.col=NA)
# par(mfrow=c(1,2))
# plot(r.hg$fitted,r.hg$residuals,xlab="Fitted values", ylab="Residuals",main="TA - diagram")
# abline(h=0,lty=2)
# qqnorm(r.hg$residuals)
# qqline(r.hg$residuals)
@
\end{center}
}


\frame{\frametitle{Linear regression is even more powerful!}
We have seen that it is possible to include continuous, binary or factorial covariates in a regression model.\\[6mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
Even \alert{transformations} of covariates can be included in (almost) any form. For instance the square of a variable $\bm{x}$
\begin{equation*}
y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \epsilon_i \ , 
\end{equation*}
which leads to a {\bf quadratic} or {\bf polynomial} regression (if higher order terms are used).
\end{minipage}}
\vspace{4mm}

Other common transformations are (see also slide \ref{sl:common}): \begin{itemize}\item $\log$ \item $\sqrt{..}$ \item $\sin$, $\cos$,... \end{itemize}
}

\frame{
How can a \emph{quadratic} regression be a \emph{linear regression}??\\[4mm]

{\bf Note}:
The word \emph{linear} refers to the \alert{linearity in the coefficients}, and not on a linear relationship between $\bm{y}$ and $\bm{x}$!\\[4mm]

\includegraphics[width=11cm]{pictures/multiplReg.pdf}
}


\frame{\frametitle{Multiple vs.\ many single regressions}

Question: Given multiple regression covariates $\bm{x}^{(1)}, \bm{x}^{(2)},...$. Could I simply fit separate simple models for each variable, that is\\[2mm]

$y_i = \alpha + \beta x_i^{(1)} + \epsilon_i$\\
$y_i = \alpha + \beta x_i^{(2)} + \epsilon_i$ \\[2mm]
etc.? \\[4mm]

\pause
Answer (Stahel 3.3o):\\[4mm]
\includegraphics[width=11cm]{pictures/citation.pdf}
~\\[4mm]

Why?
}

\frame{\frametitle{Illustration}
Chapter 3.3c in the Stahel script illustrates the point on four artificial examples. The ``correct'' model is always given as 
\begin{equation*}
y_i = \beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \epsilon_i \  ,
\end{equation*}

where $\bm{x}^{(1)}$ is a continuous variable, and $\bm{x}^{(2)}$ is a binary grouping variable (thus taking values 0 and 1 to indicate the group).\\[6mm]

Thus the correct model is 
\begin{equation*}
\begin{array}{lll}
\hat{y_i} &= \beta_0 +  \beta_1 x_i^{(1)} & \text{if $x_i^{(2)}=0$.} \\
\hat{y_i} &= \beta_0 + \beta_2 + \beta_1 x_i^{(1)}   & \text{if $x_i^{(2)}=1$.} 
\end{array}
\end{equation*}
}

\frame{
\includegraphics[width=11cm]{pictures/FigsAB.jpg}\\[2mm]

Example A: Within-group slope is $>0$. Fitting $y$ against $x$ leads to an overestimated slope when group-variable is not included in the model.\\[4mm]

Example B: Within-group slope is $0$, but fitting $y$ against $x$ leads to a slope estimate $>0$, wich is only an artefact of not accounting for the group variable $x^{(2)}$. \\[4mm]
}

\frame{
\includegraphics[width=11cm]{pictures/FigsCD.pdf}\\[2mm]

Example C: Within-group slope is $<0$, but fitting $y$ against $x$ leads to an estimated slope of $>0$! \\[4mm]

Example D: Within-group slope is $<0$, but fitting $y$ against $x$ leads to a slope estimate of $0$. \\[4mm]
}


\frame{\frametitle{Another interpretation of multiple regression}
In multiple regression, the coefficient $\beta_x$ of a covariate $x$ can be interpreted as follows:\\[4mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
$\beta_x$ explains how the response changes with $x$, while holding all the other variables constant.
\end{minipage}}

~\\[2mm]
This idea is similar in spirit to an experimental design, where the influence of a covariate of interest on the response is investigated in various environments\footnote{Clayton, D. and M. Hills (1993). Statistical Models in Epidemiology. Oxford: Oxford University Press.}. Clayton and Hills (1993) continue (p.273):\\[3mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
\begin{quote}
[...] the data analyst is in a position like that of an experimental scientist who has the capability to plan and carry out many experiments within a single day. Not surprisingly, a cool head is required!
\end{quote}
\end{minipage}}
}

\frame{\frametitle{Checking modeling assumptions}
Remember that in linear regression the modeling assumption is that the errors $\epsilon_i$ are independently normally distributed around zero, that is, $\epsilon_i \sim \N(0,\sigma^2)$. This implies four things:\\[6mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
\begin{enumerate}[a)]
\item The expected value of each residual $\epsilon_i$ is 0: $\E(\epsilon_i)=0$. (This is not the same as asumming that all residuals equal 0.\\[2mm]
\item All $\epsilon_i$ have the same variance: $\Var(\epsilon_i)=\sigma^2$. \\[2mm]
\item The $\epsilon_i$ are normally distributed.\\[2mm]
\item The $\epsilon_i$ are independent of each other.
\end{enumerate}
\end{minipage}}
~\\[5mm]
So far, we have discussed 
\begin{itemize}
\item the Tukey-Anscombe plot.
\item the QQ-plot.
\end{itemize}
}

\frame{
% Stahel 4.1b:
% \includegraphics[width=11cm]{pictures/41b.pdf}
% ~\\[6mm]

The aim is to formulate a model that describes the data well. But always keep in mind the following statement from a wise man:\\[6mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
All models are wrong, but some are useful. \scriptsize{(Box 1978)}
\end{minipage}}
}

\frame{\frametitle{Overview of model-checking tools}

Complete overview of tools used in this course:\\[2mm]

\begin{itemize}
\item Tukey-Anscombe plot (see lectures 3 and 4)\\
$\Rightarrow$ \myalert{To check assumptions a), b) and d)}\\[2mm]

\item Quantile-quantile (QQ) plot (see lectures 3 and 4)\\
$\Rightarrow$ \myalert{To check assumption c)}\\[2mm]


\item Scale-location plot (Streuungs-Diagramm)\\
$\Rightarrow$ \myalert{To check assumption b)}\\[2mm]

\item Leverage plot (Hebelarm-Diagramm)\\
$\Rightarrow$ \myalert{To find influential observations and/or outliers}\\[6mm]
\end{itemize}

{\bf Note:} these four diagrams are plotted automatically by R when you use the \texttt{plot()} or the \texttt{autoplot()}  function (from the \texttt{ggfortify} package) on an \texttt{lm} object, for example \texttt{autoplot(r.hg)}.
}

\frame[containsverbatim]{\frametitle{Tukey-Anscombe plot}
<<echo=F>>=
library(ggfortify)
path <- "../../data_examples/Hg/"
d.hg <- read.table(paste(path,"Hg_urin.csv",sep=""),header=T, sep=",")
d.hg.m <- d.hg[d.hg$mother==1,-c(3,4,9,10,11)]
names(d.hg.m) <- c("Hg_urin", "Hg_soil", "smoking","amalgam", "age", "fish")
@


It is sometimes useful to enrich the TA-plot by adding a ``running mean'' or a ``smoothed mean'', which can give hints on the trend of the residuals. For the mercury example where $\log(Hg_{\text{urin}})$ is regressed on smoking, amalgam  and fish consumption for mothers only (slides 32-34 of lecture 4): 
\vspace{-10mm}

\begin{multicols}{2} 
% \begin{center}
<<TAplot, eval=T,fig.width=4, fig.height=4,warning=FALSE,out.width="5cm">>=
r1.urin.mother <- lm(log10(Hg_urin) ~  smoking + amalgam + fish,data=d.hg.m)
plot(fitted(r1.urin.mother),residuals(r1.urin.mother),xlab="Fitted values",ylab="Residuals")
abline(h=0,lty=2)
aa <- data.frame(cbind(x=fitted(r1.urin.mother), y=residuals(r1.urin.mother)))
aa <- aa[order(aa$x),]
lo <- loess(y ~ x, aa)
lines(aa$x,predict(lo), col='red', lwd=2,lty=5)
@
\vspace{5mm}
~\\[1cm]
% \end{center}
The TA plot (again) indicates that there is an outlier in the range of -0.7 to -0.6.\\
\end{multicols}

\vspace{-5mm}
However, generally we recommend to \alert{not} add a smoothing line, because it may bias our view on the plot.

}





\frame{
The TA plot is also able to check the \emph{independence assumption} d). But how?\\[4mm]

$\rightarrow$ A dependency would be reflected by some kind of \myalert{trend}. \\[8mm]

}

% Todo: hide the following slide in the published version of the slides!!
\frame{
But: The dependency is not necessarily on the fitted values ($x$-axis of TA plot). Ideas:
\begin{itemize}
\item Plot residuals in dependency of time (if available) or sequence of obervations.
\item Plot residuals against the covariates. \\[6mm]
\end{itemize}
 
<<hgFigCov,fig.width=7, fig.height=2.5,warning=FALSE,out.width="11cm">>=
library(ggplot2)
library(gridExtra)
d1 <- data.frame(obsNr= 1:length(residuals(r1.urin.mother)), 
                 resid = residuals(r1.urin.mother),
                 amalgam = d.hg.m$amalgam,
                 fish = d.hg.m$fish)
p1 <- ggplot(d1, aes(x=obsNr,y=resid))+  geom_point() +  geom_hline(yintercept = 0,linetype=2,size=0.3,col=2) +
  xlab("Observation number (time)") +
  ylab("Residuals")
p2 <- ggplot(d1, aes(x=amalgam,y=resid))+  geom_point() +  geom_hline(yintercept = 0,linetype=2,size=0.3,col=2) +
  xlab("No. of amalgam fillings") +
  ylab("")
p3 <- ggplot(d1, aes(x=fish,y=resid))+  geom_point() +  geom_hline(yintercept = 0,linetype=2,size=0.3,col=2) +
  xlab("No. of fish meals/month") +
  ylab("")
grid.arrange(p1, p2, p3, ncol=3)

@
~\\
Again, not pattern = good.
}

\frame{\frametitle{QQ-plot}

The \myalert{outlier} recorded above is also visible in the (well-known) QQ-plot, which is useful to check for normal distribution of residuals (assumption c):\\[2mm]

\begin{center}
<<hgFigQQ, eval=T,fig.width=3.5, fig.height=3.5,warning=FALSE,out.width="5cm">>=
library(ggfortify)
autoplot(r1.urin.mother,2) + theme_bw()
# qqnorm(residuals(r1.urin.mother))
# qqline(residuals(r1.urin.mother))
@
\end{center}
}


\frame[containsverbatim]{
%\colorbox{lightgray}{\begin{minipage}{10cm}
\frametitle{How do I know if a QQ-plot looks ``good''?}
%\end{minipage}}
%\vspace{0mm}

There is {\bf no quantitative rule} to answer this question, experience is needed. However, you can gain this experience from \alert{simulations}. To this end, generate the same number of data points of a normally distributed variable and compare to your plot. \\
Example: Generate 59 points $\epsilon_i \sim \N(0,1)$ each time:\\[2mm]

\begin{center}
<<fig4, eval=T,fig.width=6.5, fig.height=4.0,warning=FALSE,out.width="8.5cm">>=
set.seed(390457)
par(mfrow=c(2,3),mar=c(4,4,1,1))
for (ii in 1:6){
  ss <- rnorm(59)
qqnorm(ss,main="")
qqline(ss,xlab="")
}
@
\end{center}
}

\frame[containsverbatim]{\frametitle{Scale-location plot (Streuungs-Diagramm)}
The scale-location plot is particularly suited to check the assumption of equal variances ({\bf homoscedasticity / Homoskedastizit\"at}).\\[4mm]

The idea is to plot the square root of the (standardized) residuals $\sqrt{|R_i|}$ against the fitted values $\hat{y_i}$. There should be {\bf no trend} {\scriptsize (Again Hg example)}: 
 
\begin{center}
<<slPlot, eval=T,fig.width=3.5, fig.height=3.5,warning=FALSE,out.width="4.8cm">>=
autoplot(r1.urin.mother,which=3) + theme_bw()

# par(mar=c(4,5,2,1))
# plot(fitted(r1.urin.mother),sqrt(abs(residuals(r1.urin.mother))),xlab="Fitted values",ylab=expression(sqrt(abs(R[i]))),cex.lab=1.2)
# aa <- data.frame(cbind(x=fitted(r1.urin.mother), y=sqrt(abs(residuals(r1.urin.mother)))))
# aa <- aa[order(aa$x),]
# lo <- loess(y ~ x, aa)
# lines(aa$x,predict(lo), col='red', lwd=2,lty=5)
@
\end{center}
}


\frame{\frametitle{Leverages (``Hebel'')}
To understand the leverage plot, we need to introduce the idea of the \emph{leverage} (``Hebel'').\\[6mm]

In simple regression, the leverage of individual $i$ is defined as $H_{ii} = (1/n) + (x_i-\overline{x})^2 SSQ^{(X)}$. Think about when leverages are expected to be large/small, and answer the two questions here:\\[2mm]

\begin{center}
\href{http://www.klicker.uzh.ch/bkx}{http://www.klicker.uzh.ch/bkx}
\end{center}

%which becomes larger the further away from the mean...\\[6mm]
}

% \frame{
% \includegraphics[width=11cm]{pictures/leverage.jpg}
% 
% }

\frame{\frametitle{Graphical illustration of the leverage effect}
Data points with $x_i$ values far from the mean have a stronger leverage effect than when $x_i\approx \overline{x}$:

\begin{center}
<<fig5, eval=T,fig.width=6.5, fig.height=2.3,warning=FALSE,out.width="9.5cm">>=
set.seed(37489)
par(mfrow=c(1,3),mar=c(4,4,1,1))
x <- sort(rnorm(18))
y <- 2*x + rnorm(18,0,0.4)
plot(y~x)
abline(lm(y~x))
y1 <- y
y1[18] <- y[18] -5
plot(y1~x,col=c(rep(1,17),2))
abline(lm(y~x))
abline(lm(y1~x),col=2,lty=2)
y2 <- y
y2[9] <- y2[9] + 4
plot(y2~x,col=c(rep(1,8),2,rep(1,9)))
abline(lm(y~x))
abline(lm(y2~x),col=2,lty=2)
@
\end{center}

The outlier in the middle plot ``pulls'' the regression line in its direction and biases the slope.\\

\hspace{2cm}$\rightarrow$ \href{http://students.brown.edu/seeing-theory/regression-analysis/index.html}{Click here} to do it manually!
}




\frame[containsverbatim]{\frametitle{Leverage plot (Hebelarm-Diagramm)}
In the leverage plot, (standardized) residuals $\tilde{R_i}$ are plotted against the leverage $H_{ii}$ (still for the Hg example):\\[2mm]
 
 
<<fig7, fig.width=3.5, fig.height=3.5,out.width="5cm">>=
#plot(r1.urin.mother,which=5)
autoplot(r1.urin.mother,which=5)
@
 
% {\scriptsize Note: Cook's distance measures how much the regression changes when the $i$th observation is omitted.}\\[2mm]
\myalert{Critical ranges} are the top and bottom right corners!!\\
Here, individuals 95, 101 and 106 are potential \myalert{outliers}.
}


\frame{\frametitle{What can go ``wrong'' during the modeling process?}

\begin{itemize}
\item ...
% \item Violation of assumptions:
% \begin{itemize}
% \item $\E(\epsilon_i)\neq 0$
% \item $\Var(\epsilon_i)$ not constant
% \item $\epsilon_i$ not normally distributed.
% \item Heteroscedasticity
% \end{itemize}
% \item Outliers
%
\end{itemize}
}

\frame{\frametitle{What to do when things go wrong?}
\begin{enumerate}[(i)]
\item \hl{Transform the outcome or the covariables.}\\[2mm]
\item \hl{Take care of outliers.}\\[2mm]
\item Use weighted regression (not discussed here).\\[2mm]
\item Improve the model, e.g., by adding additional terms or interactions (see ``model selection'' in lecture 8).\\[2mm]
\item Use another model family (generalized or nonlinear regression model).\\[2mm]
\end{enumerate}
}


\frame[containsverbatim]{\frametitle{Transformation of the response?}
Example: Use again the mercury study, include only mothers. Use the response (Hg-concentration in the urine) \myalert{without log-transformation}. What would it look like?
<<r1urin,echo=T>>=
r2.urin.mother <- lm(Hg_urin ~  smoking  + amalgam + fish,data=d.hg.m)
@
\begin{center}
<<fig8,fig.width=5.5, fig.height=5.5,out.width="6cm">>=
autoplot(r2.urin.mother) + theme_bw()
# par(mfrow=c(2,2),mar=c(4,4,2,0.5))
# plot(r2.urin.mother)
@
\end{center}
}

% \frame[containsverbatim]{
% Also the ``old-fashioned'' histogram of the residuals is illustrative, it is \myalert{very skewed}:
% \begin{center}
% <<fig9, fig.width=4, fig.height=4,out.width="5.5cm">>=
% ggplot(mapping=aes(x=residuals(r2.urin.mother)))+ geom_histogram(bins=25) + xlab("Residuals")
% # hist(residuals(r2.urin.mother),nclass=20)
% @
% \end{center}
% }

\frame[containsverbatim]{
Comparison to the model with log-transformed response:

<<r1urin.2,echo=F>>=
r3.urin.mother <- lm(log10(Hg_urin) ~  smoking  + amalgam + fish,data=d.hg.m)
@
\begin{center}
<<fig10,fig.width=5.5, fig.height=5.5,out.width="6cm">>=
autoplot(r3.urin.mother,smooth.colour=NA) + theme_bw()
# par(mfrow=c(2,2),mar=c(4,4,2,0.5))
# plot(r3.urin.mother)
@
\end{center}
This looks {\bf much} better! However... there is this individual 106 that needs some closer inspection (see slide \ref{sl:outliersHg} for the solution regarding this outlier).
}

% 
% \frame{
% A similar example is given in Stahel 4.4 a+b. The diagnostic plots of a regression where the log-transformation of the outcome was forgotten looked like this:\\
% \begin{center}
% \includegraphics[width=8cm]{pictures/logT.pdf}
% \end{center}
% }

\frame{\frametitle{Common transformations}\label{sl:common}
Which tranformations should be considered to cure model deviation symptoms? Answering this depends on plausibility and simplicity, and requires some experience. \\[6mm]

The most common and useful \myalert{first aid transformations} are:\\[4mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
\begin{itemize}
\item The log transformation for {\bf concentrations} and {\bf absolute values}.
\item The square-root ($\sqrt{\cdot}$) transformation for {\bf count data}.
\item The $\arcsin(\sqrt{\cdot})$ transformation for {\bf proportions/percentages}.
\end{itemize}
\end{minipage}}
~\\[4mm]

These transformations can (or should) also be applied on covariates!
}

\frame[containsverbatim]{\label{sl:hgSqrt}
For instance, the number of amalgam fillings and the number of monthly fish meals should be sqrt-transformed in the mercury example:\\[2mm]

<<sqrt>>=
r4.urin.mother <- lm(log10(Hg_urin) ~  smoking + sqrt(amalgam) + sqrt(fish),data=d.hg.m)
@

<<hgFig, fig.width=5.5, fig.height=5.5,out.width="7cm">>=
autoplot(r4.urin.mother,smooth.colour=NA) + theme_bw()
# par(mfrow=c(2,2),mar=c(4,4,2,0.5))
# plot(r4.urin.mother)
@

}

\frame[containsverbatim]{\frametitle{Outliers}
\vspace{-3mm}


The above plots illustrate that outliers are visible in all diagnostic plots. \\[4mm]

What to do in this case?

\begin{enumerate}
\item Start by checking the correctness of the data. Is there a typo or a digital point that was shifted by mistake? Check the covariates and the response.\\[2mm]
\item If not, ask whether the model has been misspecified. Do reasonable transformations of the response or the covariates eliminate the outlier? Do the residuals have a distribution with a long tail (which makes it more likely that extreme observations occur)?\\[2mm]
\item Sometimes, an outlier may be the most interesting observation in a dataset!\\[2mm]
\item Consider that outliers can also occur by chance!
\end{enumerate}

}

\frame{\frametitle{Deleting outliers?}
It might seem tempting to delete observations that apparently don't fit into the picture. However:\\[4mm]

\begin{itemize}
\item Do this {\bf only with absolute care}, e.g., if an observation has extremely implausible values! \\[2mm]
\item Before deleting outliers, check points 1-4 from the previous slide. \\[2mm]
\item When deleting outliers or the x\% of most extreme observations, you {\bf must mention this in your report}. \\[2mm]
\item Confidence intervals, tests and $p$-values might be biased.
\end{itemize}


}


\frame[containsverbatim]{\frametitle{The outlier in the Hg study}\label{sl:outliersHg}
In the Hg study, it turned out later on that the outlier 106 had five unreported amalgam fillings! \\[2mm]
A corrected analysis gives a much more regular picture (please compare to slide \ref{sl:hgSqrt}):
\vspace{-4mm}

<<sqrtOut,echo=F>>=
d.hg.m["106","amalgam"]<-5
r5.urin.mother <- lm(log10(Hg_urin) ~  smoking + sqrt(amalgam) + sqrt(fish),data=d.hg.m)
@
\begin{center}
<<hgFigOu, eval=T,fig.width=5, fig.height=5,warning=FALSE,out.width="6cm">>=
autoplot(r5.urin.mother,smooth.colour=NA) + theme_bw()
# par(mfrow=c(2,2),mar=c(4,4,2,0.5))
# plot(r5.urin.mother)
@
\end{center}

}


\frame{\frametitle{Feedback about today's lecture}

Please give us your opinion about the lecture regarding
\begin{itemize}
\item unclearest (``muddiest'') point
\item the take-home message of today.
\end{itemize}

via this link: \\[2mm]

\centering{\href{http://www.klicker.uzh.ch/bkx}{http://www.klicker.uzh.ch/bkx}}

~\\[2mm]
\centering{Thank you!!}
}


% \frame{References:
% \bibliographystyle{Chicago}
% \bibliography{refs}
% }



\end{document}
